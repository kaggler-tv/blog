{
  
    
        "post0": {
            "title": "Supervised Emphasized Denoising AutoEncoder",
            "content": "This notebook was originally published here at Kaggle. . . In this notebook, I will show how to build supervised emphasized Denoising AutoEncoder (DAE) with Keras. With pseudo label, we can train a classifier and the DAE together instead of training them separately as done in previous TPS competitions. . If you&#39;re interested in how different components of DAE (denoising, stacked layers, emphasis, etc.) contribute to its performance, please check out Vincent et al. (2010) &quot;Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion&quot;, JMLR. . This notebook is built on top of my previous notebook, AutoEncoder + Pseudo Label + AutoLGB. The first part (section 1, 2, 3 and 5) of the notebook is the same as the previous one. . The contents of the notebook are as follows: . Package Installation: Installing latest version of Kaggler using Pip. | Feature Engineering: code by @udbhavpangotra | Feature Transformation: Using kaggler.preprocessing.LabelEncoder to impute missing values and group rare categories automatically. | Stacked Emphasized Denoising AutoEncoder (DAE): Adding random noise mask and emphasized version of AutoEncoder, called &quot;Embphasized Denoising AutoEncoder&quot;. | LightGBM Model Training: 5-fold CV + Pseudo label from @hiro5299834&#39;s data + kaggler.model.AutoLGB&#39;s feature selection and hyperparameter optimization | Supervised DAE: Training the classifier and DAE simultaneously. | Part 1: DAE + AutoLGB . Load Libraries and Install Kaggler . # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . import lightgbm as lgb from matplotlib import pyplot as plt import numpy as np import pandas as pd from pathlib import Path import tensorflow as tf from tensorflow import keras from tensorflow.keras import backend as K from tensorflow.keras.losses import mean_squared_error from tensorflow.keras.metrics import AUC from tensorflow.python.keras.utils import control_flow_util import seaborn as sns from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import StandardScaler from sklearn.metrics import roc_auc_score, confusion_matrix import warnings . !pip install kaggler . import kaggler from kaggler.model import AutoLGB from kaggler.preprocessing import LabelEncoder print(f&#39;Kaggler: {kaggler.__version__}&#39;) print(f&#39;TensorFlow: {tf.__version__}&#39;) . warnings.simplefilter(&#39;ignore&#39;) plt.style.use(&#39;fivethirtyeight&#39;) pd.set_option(&#39;max_columns&#39;, 100) . Feature Engineering (ref: code by @udbhavpangotra) . data_dir = Path(&#39;/kaggle/input/tabular-playground-series-apr-2021/&#39;) trn_file = data_dir / &#39;train.csv&#39; tst_file = data_dir / &#39;test.csv&#39; sample_file = data_dir / &#39;sample_submission.csv&#39; pseudo_label_file = &#39;/kaggle/input/tps-apr-2021-label/voting_submission_from_5_best.csv&#39; target_col = &#39;Survived&#39; id_col = &#39;PassengerId&#39; feature_name = &#39;dae&#39; algo_name = &#39;lgb&#39; model_name = f&#39;{algo_name}_{feature_name}&#39; feature_file = f&#39;{feature_name}.csv&#39; predict_val_file = f&#39;{model_name}.val.txt&#39; predict_tst_file = f&#39;{model_name}.tst.txt&#39; submission_file = f&#39;{model_name}.sub.csv&#39; . trn = pd.read_csv(trn_file, index_col=id_col) tst = pd.read_csv(tst_file, index_col=id_col) sub = pd.read_csv(sample_file, index_col=id_col) pseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col) print(trn.shape, tst.shape, sub.shape, pseudo_label.shape) . tst[target_col] = pseudo_label[target_col] n_trn = trn.shape[0] df = pd.concat([trn, tst], axis=0) df.head() . df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;No&#39;) df[&#39;Cabin&#39;] = df[&#39;Cabin&#39;].fillna(&#39;_&#39;) df[&#39;CabinType&#39;] = df[&#39;Cabin&#39;].apply(lambda x:x[0]) df.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) &gt; 1 else &#39;X&#39;) df[&#39;Age&#39;].fillna(round(df[&#39;Age&#39;].median()), inplace=True,) df[&#39;Age&#39;] = df[&#39;Age&#39;].apply(round).astype(int) df[&#39;Fare&#39;].fillna(round(df[&#39;Fare&#39;].median()), inplace=True,) df[&#39;FirstName&#39;] = df[&#39;Name&#39;].str.split(&#39;, &#39;).str[0] df[&#39;SecondName&#39;] = df[&#39;Name&#39;].str.split(&#39;, &#39;).str[1] df[&#39;n&#39;] = 1 gb = df.groupby(&#39;FirstName&#39;) df_names = gb[&#39;n&#39;].sum() df[&#39;SameFirstName&#39;] = df[&#39;FirstName&#39;].apply(lambda x:df_names[x]) gb = df.groupby(&#39;SecondName&#39;) df_names = gb[&#39;n&#39;].sum() df[&#39;SameSecondName&#39;] = df[&#39;SecondName&#39;].apply(lambda x:df_names[x]) df[&#39;Sex&#39;] = (df[&#39;Sex&#39;] == &#39;male&#39;).astype(int) df[&#39;FamilySize&#39;] = df.SibSp + df.Parch + 1 feature_cols = [&#39;Pclass&#39;, &#39;Age&#39;,&#39;Embarked&#39;,&#39;Parch&#39;,&#39;SibSp&#39;,&#39;Fare&#39;,&#39;CabinType&#39;,&#39;Ticket&#39;,&#39;SameFirstName&#39;, &#39;SameSecondName&#39;, &#39;Sex&#39;, &#39;FamilySize&#39;, &#39;FirstName&#39;, &#39;SecondName&#39;] cat_cols = [&#39;Pclass&#39;,&#39;Embarked&#39;,&#39;CabinType&#39;,&#39;Ticket&#39;, &#39;FirstName&#39;, &#39;SecondName&#39;] num_cols = [x for x in feature_cols if x not in cat_cols] print(len(feature_cols), len(cat_cols), len(num_cols)) . Feature Transformation Using Kaggler . for col in [&#39;SameFirstName&#39;, &#39;SameSecondName&#39;, &#39;Fare&#39;, &#39;FamilySize&#39;, &#39;Parch&#39;, &#39;SibSp&#39;]: df[col] = np.log2(1 + df[col]) scaler = StandardScaler() df[num_cols] = scaler.fit_transform(df[num_cols]) lbe = LabelEncoder(min_obs=50) df[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int) . Emphasized Denoising AutoEncoder (DAE) Using Keras . encoding_dim = 128 masking_prob = .2 emphasis_ratio = 2. seed = 42 def get_dae(encoding_dim, dropout=.2): num_dim = len(num_cols) num_input = keras.layers.Input((num_dim,), name=&#39;num_input&#39;) cat_inputs = [] cat_embs = [] emb_dims = 0 for col in cat_cols: cat_input = keras.layers.Input((1,), name=f&#39;{col}_input&#39;) emb_dim = max(8, int(np.log2(1 + df[col].nunique()) * 4)) cat_emb = keras.layers.Embedding(input_dim=df[col].max() + 1, output_dim=emb_dim)(cat_input) cat_emb = keras.layers.Dropout(dropout)(cat_emb) cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb) cat_inputs.append(cat_input) cat_embs.append(cat_emb) emb_dims += emb_dim merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs) batch_size, merged_inputs_dim = merged_inputs.get_shape() training = K.learning_phase() def mask_inputs(): mask = tf.random.stateless_binomial(shape=(batch_size, merged_inputs_dim), seed=seed, counts=tf.ones((merged_inputs_dim,)), probs=[masking_prob] * merged_inputs_dim) return tf.where(mask == 1, tf.zeros_like(merged_inputs), merged_inputs) masked_inputs = control_flow_util.smart_cond(training, mask_inputs, lambda: merged_inputs) encoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(masked_inputs) encoded = keras.layers.Dropout(dropout)(encoded) encoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded) encoded = keras.layers.Dropout(dropout)(encoded) encoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded) decoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(decoded) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(num_dim + emb_dims, activation=&#39;linear&#39;)(decoded) encoder = keras.Model([num_input] + cat_inputs, encoded) ae = keras.Model([num_input] + cat_inputs, decoded, name=&#39;ae&#39;) reconstruction_loss = K.mean( # masked inputs mean_squared_error(merged_inputs, tf.where(merged_inputs != masked_inputs, decoded, merged_inputs)) / masking_prob * emphasis_ratio # original inputs + mean_squared_error(merged_inputs, tf.where(merged_inputs == masked_inputs, decoded, merged_inputs)) / (1. - masking_prob) ) ae.add_loss(reconstruction_loss) ae.compile(optimizer=&#39;adam&#39;) return ae, encoder . ae, encoder = get_dae(encoding_dim) ae.summary() . inputs = [df[num_cols].values] + [df[x].values for x in cat_cols] ae.fit(inputs, inputs, epochs=30, batch_size=16384, shuffle=True, validation_split=.2) . encoding = encoder.predict(inputs) print(encoding.shape) np.savetxt(feature_file, encoding, fmt=&#39;%.6f&#39;, delimiter=&#39;,&#39;) . Model Training + Feature Selection + HPO Using Kaggler&#39;s AutoLGB . n_fold = 5 X = pd.concat((df[feature_cols], pd.DataFrame(encoding, columns=[f&#39;enc_{x}&#39; for x in range(encoding_dim)])), axis=1) y = df[target_col] X_tst = X.iloc[n_trn:] cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed) p = np.zeros_like(y, dtype=float) p_tst = np.zeros((tst.shape[0],)) for i, (i_trn, i_val) in enumerate(cv.split(X, y)): if i == 0: clf = AutoLGB(objective=&#39;binary&#39;, metric=&#39;auc&#39;, random_state=seed) clf.tune(X.iloc[i_trn], y[i_trn]) features = clf.features params = clf.params n_best = clf.n_best print(f&#39;{n_best}&#39;) print(f&#39;{params}&#39;) print(f&#39;{features}&#39;) trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn]) val_data = lgb.Dataset(X.iloc[i_val], y[i_val]) clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100) p[i_val] = clf.predict(X.iloc[i_val]) p_tst += clf.predict(X_tst) / n_fold print(f&#39;CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}&#39;) np.savetxt(predict_val_file, p, fmt=&#39;%.6f&#39;) np.savetxt(predict_tst_file, p_tst, fmt=&#39;%.6f&#39;) . print(f&#39; CV AUC: {roc_auc_score(y, p):.6f}&#39;) print(f&#39;Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}&#39;) . Submission File for DAE + AutoLGB . n_pos = int(0.34911 * tst.shape[0]) th = sorted(p_tst, reverse=True)[n_pos] print(th) confusion_matrix(pseudo_label[target_col], (p_tst &gt; th).astype(int)) . sub[target_col] = (p_tst &gt; th).astype(int) sub.to_csv(submission_file) . Part 2: Supervised DAE . feature_name = &#39;dae&#39; algo_name = &#39;sdae&#39; model_name = f&#39;{algo_name}_{feature_name}&#39; feature_file = f&#39;{feature_name}.csv&#39; predict_val_file = f&#39;{model_name}.val.txt&#39; predict_tst_file = f&#39;{model_name}.tst.txt&#39; submission_file = f&#39;{model_name}.sub.csv&#39; . Supervised DAE with Keras . We are adding a classifier head to the DAE network. It requires the additional loss and metric for the classifier in addition to the reconstruction_loss for DAE. . def get_sdae(encoding_dim, dropout=.2): num_dim = len(num_cols) num_input = keras.layers.Input((num_dim,), name=&#39;num_input&#39;) cat_inputs = [] cat_embs = [] emb_dims = 0 for col in cat_cols: cat_input = keras.layers.Input((1,), name=f&#39;{col}_input&#39;) emb_dim = max(8, int(np.log2(1 + df[col].nunique()) * 4)) cat_emb = keras.layers.Embedding(input_dim=df[col].max() + 1, output_dim=emb_dim)(cat_input) cat_emb = keras.layers.Dropout(dropout)(cat_emb) cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb) cat_inputs.append(cat_input) cat_embs.append(cat_emb) emb_dims += emb_dim inputs = [num_input] + cat_inputs merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs) # masking batch_size, merged_inputs_dim = merged_inputs.get_shape() training = K.learning_phase() def mask_inputs(): mask = tf.random.stateless_binomial(shape=(batch_size, merged_inputs_dim), seed=seed, counts=tf.ones((merged_inputs_dim,)), probs=[masking_prob] * merged_inputs_dim) return tf.where(mask == 1, tf.zeros_like(merged_inputs), merged_inputs) masked_inputs = control_flow_util.smart_cond(training, mask_inputs, lambda: merged_inputs) # encoder encoded_1 = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(masked_inputs) encoded_1 = keras.layers.Dropout(dropout)(encoded_1) encoded_2 = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded_1) encoded_2 = keras.layers.Dropout(dropout)(encoded_2) encoded_3 = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded_2) encoded_concat = keras.layers.Concatenate()([encoded_1, encoded_2, encoded_3]) encoder = keras.Model(inputs, encoded_concat) decoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded_3) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(decoded) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(num_dim + emb_dims, activation=&#39;linear&#39;)(decoded) ae = keras.Model([num_input] + cat_inputs, decoded) # classifier clf_encoded_input = keras.Input((encoding_dim * 3,)) x = keras.layers.Dense(encoding_dim, &#39;relu&#39;)(clf_encoded_input) x = keras.layers.Dropout(dropout)(x) clf_output = keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(x) clf = keras.Model(inputs=clf_encoded_input, outputs=clf_output, name=&#39;clf&#39;) outputs = [ae(inputs), clf(encoder(inputs))] model = keras.Model(inputs, outputs, name=&#39;sdae&#39;) reconstruction_loss = K.mean( # masked inputs mean_squared_error(merged_inputs, tf.where(merged_inputs != masked_inputs, decoded, merged_inputs)) / masking_prob * emphasis_ratio # original inputs + mean_squared_error(merged_inputs, tf.where(merged_inputs == masked_inputs, decoded, merged_inputs)) / (1. - masking_prob) ) model.add_loss(reconstruction_loss) model.compile(optimizer=&#39;adam&#39;, loss={&#39;clf&#39;: &#39;binary_crossentropy&#39;}, metrics={&#39;clf&#39;: [AUC()]}) return model, encoder . sdae, encoder = get_sdae(encoding_dim) sdae.summary() . Model Training: Supervised DAE with 5-CV . n_fold = 5 X = df[feature_cols] y = df[target_col] X_tst = X.iloc[n_trn:] inputs_tst = [X_tst[num_cols].values] + [X_tst[x].values for x in cat_cols] cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed) p = np.zeros_like(y, dtype=float) p_tst = np.zeros((tst.shape[0],)) for i, (i_trn, i_val) in enumerate(cv.split(X, y)): X_trn = X.iloc[i_trn] X_val = X.iloc[i_val] inputs_trn = [X[num_cols].values[i_trn]] + [X[x].values[i_trn] for x in cat_cols] inputs_val = [X[num_cols].values[i_val]] + [X[x].values[i_val] for x in cat_cols] sdae, _ = get_sdae(encoding_dim) sdae.fit(inputs_trn, y[i_trn], epochs=20, batch_size=16384, shuffle=True, validation_data=(inputs_val, y[i_val])) p[i_val] = sdae.predict(inputs_val)[1].flatten() p_tst += sdae.predict(inputs_tst)[1].flatten() / n_fold print(f&#39;CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}&#39;) np.savetxt(predict_val_file, p, fmt=&#39;%.6f&#39;) np.savetxt(predict_tst_file, p_tst, fmt=&#39;%.6f&#39;) . print(f&#39; CV AUC: {roc_auc_score(y, p):.6f}&#39;) print(f&#39;Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}&#39;) . n_pos = int(0.34911 * tst.shape[0]) th = sorted(p_tst, reverse=True)[n_pos] print(th) confusion_matrix(pseudo_label[target_col], (p_tst &gt; th).astype(int)) . sub[target_col] = (p_tst &gt; th).astype(int) sub.to_csv(submission_file) . Part 3: Simple Ensemble . submission_file = &#39;simple_ensemble_dae.csv&#39; model_names = [&#39;lgb_dae&#39;, &#39;sdae_dae&#39;] predict_val_files = [f&#39;{x}.val.txt&#39; for x in model_names] predict_tst_files = [f&#39;{x}.tst.txt&#39; for x in model_names] dict_val_predict = {} dict_tst_predict = {} for name, val_file, tst_file in zip(model_name, predict_val_files, predict_tst_files): dict_val_predict[name] = np.loadtxt(val_file) dict_tst_predict[name] = np.loadtxt(tst_file) p = pd.DataFrame(dict_val_predict).mean(axis=1).values p_tst = pd.DataFrame(dict_tst_predict).mean(axis=1).values print(f&#39; CV AUC: {roc_auc_score(y, p):.6f}&#39;) print(f&#39;Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}&#39;) . n_pos = int(0.34911 * tst.shape[0]) th = sorted(p_tst, reverse=True)[n_pos] print(th) confusion_matrix(pseudo_label[target_col], (p_tst &gt; th).astype(int)) . sub[target_col] = (p_tst &gt; th).astype(int) sub.to_csv(submission_file) . If you find it helpful, please upvote the notebook and give a star to Kaggler. If you have questions and/or feature requests for Kaggler, please post them as Issue in the Kaggler GitHub repository. . Happy Kaggling! .",
            "url": "kaggler.com/notebook/kaggle/2021/04/21/supervised-emphasized-denoising-autoencoder.html",
            "relUrl": "/notebook/kaggle/2021/04/21/supervised-emphasized-denoising-autoencoder.html",
            "date": " ‚Ä¢ Apr 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AutoEncoder + Pseudo Label + AutoLGB",
            "content": "This notebook was originally published here at Kaggle. . . In this notebook, I will show how to use autoencoder, feature selection, hyperparameter optimization, and pseudo labeling using the Keras and Kaggler Python packages. . The contents of the notebook are as follows: . Package installation: Installing latest version of Kaggler using Pip | Regular feature engineering: code by @udbhavpangotra | Feature transformation: Using kaggler.preprocessing.LabelEncoder to impute missing values and group rare categories automatically. | Stacked AutoEncoder: Notebooks for DAE will be shared later. | Model training: with 5-fold CV and pseudo label from @hiro5299834&#39;s data. | Feature selection and hyperparameter optimization: Using kaggler.model.AutoLGB | Saving a submission file | Load libraries and install Kaggler . # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . %matplotlib inline import lightgbm as lgb from matplotlib import pyplot as plt import numpy as np import pandas as pd from pathlib import Path import tensorflow as tf from tensorflow import keras import seaborn as sns from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import StandardScaler from sklearn.metrics import roc_auc_score, confusion_matrix import warnings . !pip install kaggler . import kaggler from kaggler.model import AutoLGB from kaggler.preprocessing import LabelEncoder print(f&#39;Kaggler: {kaggler.__version__}&#39;) print(f&#39;TensorFlow: {tf.__version__}&#39;) . warnings.simplefilter(&#39;ignore&#39;) plt.style.use(&#39;fivethirtyeight&#39;) pd.set_option(&#39;max_columns&#39;, 100) . Feature Engineering (ref: code by @udbhavpangotra) . feature_name = &#39;ae&#39; algo_name = &#39;lgb&#39; model_name = f&#39;{algo_name}_{feature_name}&#39; data_dir = Path(&#39;/kaggle/input/tabular-playground-series-apr-2021/&#39;) trn_file = data_dir / &#39;train.csv&#39; tst_file = data_dir / &#39;test.csv&#39; sample_file = data_dir / &#39;sample_submission.csv&#39; pseudo_label_file = &#39;/kaggle/input/tps-apr-2021-label/voting_submission_from_5_best.csv&#39; feature_file = f&#39;{feature_name}.csv&#39; predict_val_file = f&#39;{model_name}.val.txt&#39; predict_tst_file = f&#39;{model_name}.tst.txt&#39; submission_file = f&#39;{model_name}.sub.csv&#39; target_col = &#39;Survived&#39; id_col = &#39;PassengerId&#39; . trn = pd.read_csv(trn_file, index_col=id_col) tst = pd.read_csv(tst_file, index_col=id_col) sub = pd.read_csv(sample_file, index_col=id_col) pseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col) print(trn.shape, tst.shape, sub.shape, pseudo_label.shape) . tst[target_col] = pseudo_label[target_col] n_trn = trn.shape[0] df = pd.concat([trn, tst], axis=0) df.head() . df.info() . df.describe() . df.nunique() . df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;No&#39;) df[&#39;Cabin&#39;] = df[&#39;Cabin&#39;].fillna(&#39;_&#39;) df[&#39;CabinType&#39;] = df[&#39;Cabin&#39;].apply(lambda x:x[0]) df.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) &gt; 1 else &#39;X&#39;) df[&#39;Age&#39;].fillna(round(df[&#39;Age&#39;].median()), inplace=True,) df[&#39;Age&#39;] = df[&#39;Age&#39;].apply(round).astype(int) df[&#39;Fare&#39;].fillna(round(df[&#39;Fare&#39;].median()), inplace=True,) df[&#39;FirstName&#39;] = df[&#39;Name&#39;].str.split(&#39;, &#39;).str[0] df[&#39;SecondName&#39;] = df[&#39;Name&#39;].str.split(&#39;, &#39;).str[1] df[&#39;n&#39;] = 1 gb = df.groupby(&#39;FirstName&#39;) df_names = gb[&#39;n&#39;].sum() df[&#39;SameFirstName&#39;] = df[&#39;FirstName&#39;].apply(lambda x:df_names[x]) gb = df.groupby(&#39;SecondName&#39;) df_names = gb[&#39;n&#39;].sum() df[&#39;SameSecondName&#39;] = df[&#39;SecondName&#39;].apply(lambda x:df_names[x]) df[&#39;Sex&#39;] = (df[&#39;Sex&#39;] == &#39;male&#39;).astype(int) df[&#39;FamilySize&#39;] = df.SibSp + df.Parch + 1 feature_cols = [&#39;Pclass&#39;, &#39;Age&#39;,&#39;Embarked&#39;,&#39;Parch&#39;,&#39;SibSp&#39;,&#39;Fare&#39;,&#39;CabinType&#39;,&#39;Ticket&#39;,&#39;SameFirstName&#39;, &#39;SameSecondName&#39;, &#39;Sex&#39;, &#39;FamilySize&#39;, &#39;FirstName&#39;, &#39;SecondName&#39;] cat_cols = [&#39;Pclass&#39;,&#39;Embarked&#39;,&#39;CabinType&#39;,&#39;Ticket&#39;, &#39;FirstName&#39;, &#39;SecondName&#39;] num_cols = [x for x in feature_cols if x not in cat_cols] print(len(feature_cols), len(cat_cols), len(num_cols)) . df[num_cols].describe() . plt.figure(figsize=(16, 16)) for i, col in enumerate(num_cols): ax = plt.subplot(4, 2, i + 1) ax.set_title(col) df[col].hist(bins=50) . Feature Transformation . Apply log2(1 + x) transformation followed by standardization for count variables to make them close to the normal distribution. log2(1 + x) has better resolution than log1p and it preserves the values of 0 and 1. . for col in [&#39;SameFirstName&#39;, &#39;SameSecondName&#39;, &#39;Fare&#39;, &#39;FamilySize&#39;, &#39;Parch&#39;, &#39;SibSp&#39;]: df[col] = np.log2(1 + df[col]) df.describe() . scaler = StandardScaler() df[num_cols] = scaler.fit_transform(df[num_cols]) . Label-encode categorical variables using kaggler.preprocessing.LabelEncoder, which creates new categories for NaNs as well as rare categories (using the threshold of min_obs). . lbe = LabelEncoder(min_obs=50) df[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int) . AutoEncoder using Keras . Basic stacked autoencoder. I will add the versions with DAE and emphasized DAE later. . encoding_dim = 64 def get_model(encoding_dim, dropout=.2): num_dim = len(num_cols) num_input = keras.layers.Input((num_dim,), name=&#39;num_input&#39;) cat_inputs = [] cat_embs = [] emb_dims = 0 for col in cat_cols: cat_input = keras.layers.Input((1,), name=f&#39;{col}_input&#39;) emb_dim = max(8, int(np.log2(1 + df[col].nunique()) * 4)) cat_emb = keras.layers.Embedding(input_dim=df[col].max() + 1, output_dim=emb_dim)(cat_input) cat_emb = keras.layers.Dropout(dropout)(cat_emb) cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb) cat_inputs.append(cat_input) cat_embs.append(cat_emb) emb_dims += emb_dim merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs) encoded = keras.layers.Dense(encoding_dim * 3, activation=&#39;relu&#39;)(merged_inputs) encoded = keras.layers.Dropout(dropout)(encoded) encoded = keras.layers.Dense(encoding_dim * 2, activation=&#39;relu&#39;)(encoded) encoded = keras.layers.Dropout(dropout)(encoded) encoded = keras.layers.Dense(encoding_dim, activation=&#39;relu&#39;)(encoded) decoded = keras.layers.Dense(encoding_dim * 2, activation=&#39;relu&#39;)(encoded) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(encoding_dim * 3, activation=&#39;relu&#39;)(decoded) decoded = keras.layers.Dropout(dropout)(decoded) decoded = keras.layers.Dense(num_dim + emb_dims, activation=&#39;linear&#39;)(decoded) encoder = keras.Model([num_input] + cat_inputs, encoded) ae = keras.Model([num_input] + cat_inputs, decoded) ae.add_loss(keras.losses.mean_squared_error(merged_inputs, decoded)) ae.compile(optimizer=&#39;adam&#39;) return ae, encoder . ae, encoder = get_model(encoding_dim) ae.summary() . inputs = [df[num_cols].values] + [df[x].values for x in cat_cols] ae.fit(inputs, inputs, epochs=100, batch_size=16384, shuffle=True, validation_split=.2) . encoding = encoder.predict(inputs) print(encoding.shape) np.savetxt(feature_file, encoding, fmt=&#39;%.6f&#39;, delimiter=&#39;,&#39;) . Model Training + Feature Selection + Hyperparameter Optimization . Train the LightGBM model with pseudo label and 5-fold CV. In the first fold, perform feature selection and hyperparameter optimization using kaggler.model.AutoLGB. . seed = 42 n_fold = 5 X = pd.concat((df[feature_cols], pd.DataFrame(encoding, columns=[f&#39;enc_{x}&#39; for x in range(encoding_dim)])), axis=1) y = df[target_col] X_tst = X.iloc[n_trn:] cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed) p = np.zeros_like(y, dtype=float) p_tst = np.zeros((tst.shape[0],)) for i, (i_trn, i_val) in enumerate(cv.split(X, y)): if i == 0: clf = AutoLGB(objective=&#39;binary&#39;, metric=&#39;auc&#39;, random_state=seed) clf.tune(X.iloc[i_trn], y[i_trn]) features = clf.features params = clf.params n_best = clf.n_best print(f&#39;{n_best}&#39;) print(f&#39;{params}&#39;) print(f&#39;{features}&#39;) trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn]) val_data = lgb.Dataset(X.iloc[i_val], y[i_val]) clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100) p[i_val] = clf.predict(X.iloc[i_val]) p_tst += clf.predict(X_tst) / n_fold print(f&#39;CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}&#39;) np.savetxt(predict_val_file, p, fmt=&#39;%.6f&#39;) np.savetxt(predict_tst_file, p_tst, fmt=&#39;%.6f&#39;) . print(f&#39; CV AUC: {roc_auc_score(y, p):.6f}&#39;) print(f&#39;Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}&#39;) . Submission File . n_pos = int(0.34911 * tst.shape[0]) th = sorted(p_tst, reverse=True)[n_pos] print(th) confusion_matrix(pseudo_label[target_col], (p_tst &gt; th).astype(int)) . sub[target_col] = (p_tst &gt; th).astype(int) sub.to_csv(submission_file) . If you find it helpful, please upvote the notebook and give a star to Kaggler. If you have questions and/or feature requests for Kaggler, please post them as Issue in the Kaggler GitHub repository. . Happy Kaggling! .",
            "url": "kaggler.com/notebook/kaggle/2021/04/18/autoencoder-pseudo-label-autolgb.html",
            "relUrl": "/notebook/kaggle/2021/04/18/autoencoder-pseudo-label-autolgb.html",
            "date": " ‚Ä¢ Apr 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "kaggler.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Kaggler Pipeline",
            "content": "Kaggler Pipeline for Data Science Competitions . In this blog, we are going to go over the fundementals of the Kaggler repository, a machine learning pipeline for data science competitions. The Kaggler pipeline uses Makefiles and Python scripts to coordinate dependencies, and allows quick iteration of new features and models. You can watch the demo at Kaggler TV Episode #4. . The pipeline is driven by running a model training, such as logistic regression, using the corresponding Makefile, e.g., $make -f Makefile.logreg1. Before going into the details of a model run, let‚Äôs build our data and features from bottom up. To start, we need to initialize our repo by going to the Kaggler repository and clicking use this template to name and create our repository, e.g., cat-in-the-dat-ii. Next, clone the repository by . $git clone https://github.com/YOUR_GITHUB_ID/cat-in-the-dat-ii.git $cd cat-in-the-dat-ii . Data . The file Makefile defines the directories and the structure of the pipeline. . # XXX: competition name COMPETITION := cat-in-the-dat-ii # gsed on macOS. sed on LINUX SED := gsed # directories DIR_DATA := input DIR_BUILD := build DIR_FEATURE := $(DIR_BUILD)/feature DIR_METRIC := $(DIR_BUILD)/metric DIR_MODEL := $(DIR_BUILD)/model # directories for the cross validation and ensembling DIR_VAL := $(DIR_BUILD)/val DIR_TST := $(DIR_BUILD)/tst DIR_SUB := $(DIR_BUILD)/sub DIRS := $(DIR_DATA) $(DIR_BUILD) $(DIR_FEATURE) $(DIR_METRIC) $(DIR_MODEL) $(DIR_VAL) $(DIR_TST) $(DIR_SUB) # data files for training and predict DATA_TRN := $(DIR_DATA)/train.csv DATA_TST := $(DIR_DATA)/test.csv SAMPLE_SUBMISSION := $(DIR_DATA)/sample_submission.csv LABEL_IDX = 25 ID_TST := $(DIR_DATA)/id.tst.csv HEADER := $(DIR_DATA)/header.csv Y_TRN:= $(DIR_FEATURE)/y.trn.txt Y_TST:= $(DIR_FEATURE)/y.tst.txt data: $(DATA_TRN) $(DATA_TST) $(SAMPLE_SUBMISSION) $(DIRS): mkdir -p $@ $(DATA_TRN) $(DATA_TST) $(SAMPLE_SUBMISSION): | $(DIR_DATA) kaggle competitions download -c $(COMPETITION) -p $(DIR_DATA) find . -name &quot;*.zip&quot; -exec sh -c &#39;unzip -d `dirname {}` {}&#39; &#39;;&#39; $(HEADER): $(SAMPLE_SUBMISSION) head -1 $&lt; &gt; $@ $(ID_TST): $(SAMPLE_SUBMISSION) cut -d, -f1 $&lt; | tail -n +2 &gt; $@ $(Y_TST): $(SAMPLE_SUBMISSION) | $(DIR_FEATURE) cut -d, -f2 $&lt; | tail -n +2 &gt; $@ $(Y_TRN): $(DATA_TRN) | $(DIR_FEATURE) cut -d, -f$(LABEL_IDX) $&lt; | tail -n +2 &gt; $@ # cleanup clean:: find . -name &#39;*.pyc&#39; -delete clobber: clean -rm -rf $(DIR_DATA) $(DIR_BUILD) .PHONY: clean clobber mac.setup ubuntu.setup apt.setup pip.setup . First, we need to define the name of the competition in Makefile. After that, running $make data will download the specified competition data from Kaggle into the ./input directory. You need to install the Kaggle API, and accept the competition rules on Kaggle to be able to download the data. If you do not download the data manually at this time, the pipeline will automatically start the download when running the first model training. The parameters defined in Makefile are as follows. . $DIR_DATA is the directory for the input data. . $DIR_TRN, $DIR_TST, and $SAMPLE_SUBMISSION are the downloaded train, test and sample submission files. . LABEL_IDX is the column index of the target variable in the train file, and needs to be specified. . $Y_TRN is the file containing the target labels, and it is created automatically by the pipeline. . $HEADER and $ID_TST are also created by the pipeline, and are used to build submission files. . Feature Engineering . Let‚Äôs create our first feature by one hot encoding all the categorical columns 1. All the columns in this competition are categorical. The feature engieering for a specific feature are defined in files ./src/generate_$FEATURE_NAME.py, e.g., ./src/generate_e1.py. We also need to create a makefile correponding to this feature, Makefile.feature.e1 as follows. . #-- # e1: all OHE&#39;d features #-- include Makefile FEATURE_NAME := e1 FEATURE_TRN := $(DIR_FEATURE)/$(FEATURE_NAME).trn.sps FEATURE_TST := $(DIR_FEATURE)/$(FEATURE_NAME).tst.sps FEATURE_MAP := $(DIR_FEATURE)/$(FEATURE_NAME).fmap $(FEATURE_TRN) $(FEATURE_TST) $(FEATURE_MAP): $(DATA_TRN) $(DATA_TST) | $(DIR_FEATURE) python ./src/generate_$(FEATURE_NAME).py --train-file $&lt; --test-file $(lastword $^) --train-feature-file $(FEATURE_TRN) --test-feature-file $(FEATURE_TST) --feature-map-file $(FEATURE_MAP) . Feature makefiles include all the parameters from Makefile. The parameters defined in Makefile.feature.e1 are . FEATURE_NAME: specified name of the feature . FEATURE_TRN, FEATURE_TST: train and test feature files, which are the outputs created by ./src/generate_$(FEATURE_NAME).py. . FEATURE_MAP: a file where we keep the name of the features, which is also created by ./src/generate_$(FEATURE_NAME).py. . Cross-Validation and Test Predictions . The models are defined in makefiles Makefile.$ALGO_NAME, e.g., Makefile.logreg1. At the top of each model file, we define which feature is going to be included as shown below. Then, we give the algorithm a short name, ALGO_NAME, for reference. We define the parameters for the algorithm, C, REGULARIZER, CLASS_WEIGHT and SOLVER in this case. We also specifiy a model name for reference, MODEL_NAME. The cross validation for algorithms are run by files ./src/train_predict_$MODEL_NAME.py, e.g., ./src/train_predict_logreg1.py, which produce validation and test predictions, PREDICT_VAL and PREDICT_TST. . After the cross validation, ./src/evaluate.py evaluates the validation predictions for a given metric, and writes the score to the file METRIC_VAL. Finally the submission file, SUBMISSION_TST, is created using the test predictions. . include Makefile.feature.e1 ALGO_NAME := logreg C := 1.0 REGULARIZER := l2 CLASS_WEIGHT := balanced SOLVER := lbfgs MODEL_NAME := $(FEATURE_NAME)_$(ALGO_NAME)_$(REGULARIZER)_$(C) METRIC_VAL := $(DIR_METRIC)/$(MODEL_NAME).val.txt PREDICT_VAL := $(DIR_VAL)/$(MODEL_NAME).val.yht PREDICT_TST := $(DIR_TST)/$(MODEL_NAME).tst.yht SUBMISSION_TST := $(DIR_SUB)/$(MODEL_NAME)_sub.csv all: validation submission validation: $(METRIC_VAL) submission: $(SUBMISSION_TST) retrain: clean_$(ALGO_NAME) submission submit: $(SUBMISSION_TST) kaggle competitions submit -c $(COMPETITION) -f $&lt; -m $(MODEL_NAME) $(PREDICT_TST) $(PREDICT_VAL): $(FEATURE_TRN) $(FEATURE_TST) | $(DIR_VAL) $(DIR_TST) python ./src/train_predict_logreg1.py --train-feature-file $&lt; --test-feature-file $(word 2, $^) --predict-valid-file $(PREDICT_VAL) --predict-test-file $(PREDICT_TST) --C $(C) --regularizer $(REGULARIZER) --class_weight $(CLASS_WEIGHT) --solver $(SOLVER) --retrain $(METRIC_VAL): $(PREDICT_VAL) $(Y_TRN) | $(DIR_METRIC) python ./src/evaluate.py --predict-file $&lt; --target-file $(lastword $^) &gt; $@ cat $@ $(SUBMISSION_TST): $(PREDICT_TST) $(HEADER) $(ID_TST) | $(DIR_SUB) paste -d, $(lastword $^) $&lt; &gt; $@.tmp cat $(word 2, $^) $@.tmp &gt; $@ rm $@.tmp .DEFAULT_GOAL := all . If we would like to run the same model with a different feature, e.g., j1, all we need to do is to change the first line in Makefile.logreg1 to include Makefile.feature.j1. The pipeline will automatically create this feature and run cross validation using ./src/train_predict_j1.py. . Similarly, if we would like to run cross-validation using a different model, such as LightGBM, we need to include the right feature in Makefile.lgb1, and run $make -f Makefile.lgb1. If the train and test features for are already created, they will not be created again. . Ensemble . After creating several features and model runs, running the ensemble model is similar to running a single model. Ensemble model uses the predictions from single model runs as features. All we need to do is specify which model predictions should be included in the ensemble in Makefile.feature.esb1 as base models. The feature names should be the same as the model names defined in the model makefiles. . Submit . Final step is to submit our predictions. Kaggler pipeline allows submitting predictions through CLI. You need to have the following lines in your model makefile, as in Makefile.lgb1 . submit: $(SUBMISSION_TST) kaggle competitions submit -c $(COMPETITION) -f $&lt; -m $(MODEL_NAME) . To make a submission with the predictions from this model and feature, all you need to do is type the following. The submission will inculde MODEL_NAME as a message for the submission. . $make -f Makefile.lgb1 submit . Conclusion . We covered the main components of the Kaggler repository. Hopefully, this blog helps you become more comfortable with the Kaggler pipeline. Happy Kaggling :) . References . Kaggler repository: https://github.com/kaggler-tv/kaggler-template . | Kaggler-TV Episode 4: https://www.youtube.com/watch?v=861NAO5-XJo&amp;feature=youtu.be . | Official Kaggle_API: https://github.com/Kaggle/kaggle-api . | Kaggler template for cat-in-the-dat-ii: https://github.com/kaggler-tv/cat-in-the-dat-ii . | https://www.kaggle.com/cuijamm/simple-onehot-logisticregression-score-0-80801 . | https://www.kaggle.com/cuijamm/simple-onehot-logisticregression-score-0-80801¬†&#8617; . |",
            "url": "kaggler.com/2020/02/19/kaggler-pipeline.html",
            "relUrl": "/2020/02/19/kaggler-pipeline.html",
            "date": " ‚Ä¢ Feb 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "kaggler.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Kaggler 0 8",
            "content": "Kaggler 0.8.0 Release . Kaggler 0.8.0 is released. It added model.BaseAutoML and model.AutoLGB for automatic feature selection and hyper-parameter tuning using hyperopt. . The implementation is based on the solution of the team AvengersEnsmbl at the KDD Cup 2019 Auto ML track. Details and winners‚Äô solutions at the competition are available at the competition website. . model.BaseAutoML is the base class, from which you can inherit to implement your own auto ML class. model.AutoLGB is the auto ML class for LightGBM. It‚Äôs simple to use as follows: . from kaggler.model import AutoLGB model = AutoLGB(objective=&#39;binary&#39;, metric=&#39;auc&#39;) model.tune(X_trn, y_trn) model.fit(X_trn, y_trn) p = model.predict(X_tst) . Other updates include: . Add .travis.yml for Travis CI | Add tests with pytest | Use flake8 linting | Fix the macOS installation issue (#34) | For more details, please check out the documentation and repository. | . Any comments or contributions will be appreciated. .",
            "url": "kaggler.com/2019/08/03/kaggler-0-8.html",
            "relUrl": "/2019/08/03/kaggler-0-8.html",
            "date": " ‚Ä¢ Aug 3, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Layoff My Story",
            "content": "Layoff ‚Äì My Story . Update: I was not affected by the layoff in the news. I‚Äôm sharing about the one I had a while ago. I don‚Äôt need a new opportunity for now. Thanks for asking. üôÇ . . . It was one of less ideal days at work. . While getting ready for work, I received an email announcing the layoff of over 400 people in marketing. . At work, a series of follow-up meetings got scheduled. I talked to my team, who didn‚Äôt get affected, to address any concerns. . Later in the afternoon, farewell emails began to arrive. I replied to some, wished the best, and connected to them on LinkedIn. . It surely hurts when it happens. Either to me or to my colleagues. . . I was laid off in 2001. It was just 9 months after I started my first full time job when the company shut down my department. I was confused, angry and felt like a failure, then I cried. . Looking back, I am thankful for the experience. . It opened up better opportunities. With the previous work experience, the job search was much easier. I had better understanding of what I could offer, and which companies needed it. In the end, I landed at a company with a greater fit. . It helped me have the right mindset at work. At the new company, I grew so much and so fast because I was humbled and grateful for the new opportunity every day. . It also gave me a better perspective on my career. Since then, no matter how I like my job, I keep reminding myself that ‚Äúthis won‚Äôt be my last company‚Äú. Employment can change, but relationship, skills and experiences will last. It‚Äôs wise to focus on what will last. . . If it happens again, it will still hurt, but I won‚Äôt feel the same confusion, anger or failure. I won‚Äôt cry. üôÇ . I hope all my colleagues keep their heads up, get stronger and wiser, and find better opportunities. . Best wishes for their career and families. .",
            "url": "kaggler.com/2019/07/30/layoff-my-story.html",
            "relUrl": "/2019/07/30/layoff-my-story.html",
            "date": " ‚Ä¢ Jul 30, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Neurips 2017 Notes",
            "content": "NeurIPS 2017 Notes . by Hang Li . Jeong and I attended NeurIPS 2017 in December, 2017. Our notes are as follows. . NeurIPS 2017 Notes | Take-Aways for Professionals | Technical Trends | Detailed Session-by-Session Notes On 12/4 (Mon) TUTORIALS | OPENING REMARKS/ INVITED TALK | POSTER SESSIONS | | On 12/5 (Tue) INVITED TALK | POSTER SESSIONS | | On 12/6 (Wed) INVITED TALK | POSTER SESSIONS | | On 12/7 (Thu) INVITED TALK | SYMPOSIUM ‚Äì INTERPRETABLE ML | | | Other Resources | Take-Aways for Professionals . As shown in the statistics shared by organizers during opening remarks, the majority of NeurIPS papers are from academia. Even papers from industry, which are only a small fraction, are mostly from research organizations. What can professionals take away from this academic conference? In my experience, people from industry can get following benefits from NeurIPS. . Cutting-edge research: This might not be applicable in practice immediately, but can still provide important perspectives and directions on each problem. | Recruiting: I would say that 90% of sponsors are focus on hiring. All big companies had their after-parties (a.k.a. recruiting events). | Networking: For some people, this is the most important benefit at NeurIPS. With over 7,000 attendees, NeurIPS 2017 was the largest academic conference in Machine Learning. Everyday I enjoyed conversations with many people in the same field at the poster sessions, after-parties, and even on the way back home with uberpool. | . . Technical Trends . We noticed technical trends as follows. . Meta-learning | Interpretability | ML systems (or systems for ML) | Bayesian modeling | Unsupervised learning | Probabilistic programming | . Below are areas that I would like to investigate further in 2018. . Model Interpretation | Attention models | Online learning | Reinforcement learning | . Detailed Session-by-Session Notes . Below are more detailed notes: . On 12/4 (Mon) . TUTORIALS . Title Comments . Deep Learning: Practice and Trends | Very good summary of deep learning‚Äôs current status and trends. CNN, RNN, adversarial networks and unsupervised domain adaptation are closer to actual application. These models should be in professionals‚Äô tool boxes. Meta learning and graph networks are interesting but further away from application. | . Deep Probabilistic Modeling with Gaussian Processes | This talk brings an important point. In real world applications, we need to know not only pointwise predictions, but also the level of uncertainty in predictions to support decision making. | . Geometric Deep Learning on Graphs and Manifolds by Michael Bronstein | This talk focuses on a interesting trend in deep learning, which uses deep learning on graphs data. In my opinion, there is still a long way to have real applications out of this field. | . OPENING REMARKS/ INVITED TALK . | Title | Comments | | ‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî‚Äî‚Äî- | | Opening Remarks &amp; Powering the next 100 years | Opening remarks has several interesting statistics of NeurIPS. It shows that NeurIPS is a very academia-centric conference. The invited talk, explains the huge amount of energy human need and the limitation of fossil fuel and low-carbon tech. Some ideas of how machine learning can help new energy (fusion) next 100 years and have big impact. Including: exploration and inference experiments data. Adding human (domain experts) preferences into ML approach. Mentioned several Bayesian approaches. It is about applied machine learning in physics which can impact world a lot. Thanks to many open source frameworks, it gets much easier to apply ML to different problem. ML becomes a major tool and will have huge impact across different domains. | . POSTER SESSIONS . Title Comments . SvCCa: Singular vector Canonical Correlation analysis for Deep understanding and improvement | Google‚Äôs blog and paper to understand deep learning models. It can be used to improve prediction performance. The key idea is using Singular vector Canonical Correlation (SvCC) to analysis hidden layer parameters. | . Dropoutnet: addressing Cold Start in recommender Systems | This focuses only on the item cold start. It need a metadata based vector representative of new items. | . LightGBM: A Highly Efficient Gradient Boosting Decision Tree | This paper explains the implementation of LightGBM. It uses different approximate approach from XGBoost‚Äôs. | . Discovering Potential Correlations via Hypercontractivity | An interesting idea to find potential relationship in the subset of data. | . Other interesting papers | Learning Hierarchical Information Flow with Recurrent Neural Modules. Learning ReLUs via Gradient Descent. Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems | . On 12/5 (Tue) . INVITED TALK . Title Comments . Why AI Will Make it Possible to Reprogram the Human Genome | This is one of the most impactful areas of AI/DL. Lately, AI/DL has been used to tackle many challenges in healthcare and shown some promising results. | . Test Of Time Award: Random Features for Large-Scale Kernel Machines | This is the spotlight talk of NeurIPS 2017. It stirred a lot of discussions online. I highly recommend that you watch the video. Points from both sides of discussion are valid. Some related discussions: Yann LeCun‚Äôs rebuttal to Ali‚Äôs talk Alchemy, Rigour and Engineering | . The Trouble with Bias | This is a good topic. Data collection and creation process can introduce strong undesirable bias to the data set. ML algorithms can reproduce and even reinforce such bias. This is more than a technical problem. | . POSTER SESSIONS . Title Comments . A Unified Approach to Interpreting Model Predictions | Use expectations and Shapley values to interpret model prediction. Unified several previous approaches including LIME. https://github.com/slundberg/shap | . Positive-Unlabeled Learning with Non-Negative Risk Estimator | 1 class classification is very useful in real world, e.g. click ads, watch content, etc. This paper use a different loss function in PU learning. | . An Applied Algorithmic Foundation for Hierarchical Clustering | There are several papers on hierarchical clustering. This is just one of them. Hierarchical clustering is also very useful in real world. In this paper it more focus on the foundation(objective function) of this problem. | . Affinity Clustering: Hierarchical Clustering at Scale | Another hierarchical clustering paper. A bottom-up hierarchical clustering. Each time make many merge decisions. | . Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results | This is an interesting semi-supervised deep learning approach. I feel it used students to prevent overfitting. Teacher and student improve each other in a virtuous cycle. | . Unbiased estimates for linear regression via volume sampling | Choose samples wisely can get similar (not bad) performance w entire data set. This will be useful in the scenarios which is costly to get labels. | . A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control | There are several papers of MAB(Multi-armed bandit), this is one of them. MAB can be very useful in website optimization. | . Other interesting papers | Streaming Weak Submodularity: Interpreting Neural Networks on the Fly. Generalization Properties of Learning with Random Features | . On 12/6 (Wed) . INVITED TALK . Title Comments . The Unreasonable Effectiveness of Structure | This talk discussed the structure in input and output. Then describe a way to describe ‚Äústructure‚Äù in data. (Probabilistic Soft Logic http://psl.linqs.org/ ) | . Deep Learning for Robotics | If working in robotics domain, this is a must attend talk. This talk discussed many unsolved pieces to the AI robotics puzzle and how DL (deep reinforcement learning, meta learning, etc ) can help. Some ideas might be useful in other domain. | . POSTER SESSIONS . Title Comments . Clustering with Noisy Queries | This paper describe and analysis a way of how to gather answers of a clustering problem. Instead of asking ‚Äúdo element u belong to cluster A‚Äù this paper suggest asking ‚Äúdo elements u and v belong to the same cluster?‚Äù | . End-to-End Differentiable Proving | Very interesting paper which try to combine NN and 1st order logic expert system. Learn vector representation of symbols. | . ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games | Looks like a fun place to try AI(:)). | . Attention Is All You Need | A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. | . Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles | Measure the uncertainty is very important. This paper describe a way (simple non-Bayesian baseline) to measure uncertainty. | . Other interesting papers | Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Unsupervised Image-to-Image Translation Networks. A simple neural network module for relational reasoning. Style Transfer from Non-parallel Text by Cross-Alignment | . On 12/7 (Thu) . INVITED TALK . Title Comments . Learning State Representations | This is a very interesting talk. It tried to peel the onions of how human make decision and learn stuff. The researcher also design experiments to prove the hypothesis of ‚Äúwe cluster experiences together into task states based on similarity and learning happens within a cluster, not across cluster borders‚Äù. Then try to design model structure to represent this cluster(state). | . On Bayesian Deep Learning and Deep Bayesian Learning | This talk is about combine Bayesian Learning and Deep Learning. This topic can be very useful in the future. It also include several projects in this area. | . SYMPOSIUM ‚Äì INTERPRETABLE ML . Title Comments . About this symposium | I think interpretability is a very important part of models. As be mentioned in one talk of this symposium interpretability is not a purely computational problem and beyond tech. The final goal still be untangle(understand) causal impact, model interpretability can be valuable in at least 2 aspects: debug model predict, help generate hypotheses to do controlled experiment. | . Invited talk - The role of causality for interpretability. | This talk discussed how to use causality in model interpretability. | . Invited talk - Interpretable Discovery in Large Image Data Sets | This talk present a DEMUD(SVOD-based plus explanations) method to interprete image data sets. | . Poster | Detecting Bias in Black-Box Models Using Transparent Model Distillation. The Intriguing Properties of Model Explanations. Feature importance scores and lossless feature pruning using Banzhaf power indices | . Debate about whether or not interpretability is necessary for machine learning | Interesting debates about interpretability. Worth to watch. | . Other Resources . NeurIPS videos, slides and notes are available as follows. . NeurIPS 2017 Proceedings | Slides | Videos https://www.youtube.com/results?search_query=NeurIPS+2017 | https://nips.cc/Conferences/2017/Videos | . | Curated Resources https://github.com/kihosuh/nips_2017 | https://github.com/hindupuravinash/nips2017 | . | Notes NeurIPS 2017 ‚Äì Day 1 Highlights by Emmanuel Ameisen | NeurIPS 2017 ‚Äì Day 2 Highlights by Emmanuel Ameisen | NeurIPS 2017 ‚Äì Day 3 Highlights by Emmanuel Ameisen | Highlights from My First NeurIPS by Ryan Rosario | NeurIPS 2017 Notes by David Abel (pdf) | NeurIPS 2017 Reports by Viktoriya Krakovna | NeurIPS 2017 notes and thoughts by Olga Liakhovich | . | .",
            "url": "kaggler.com/2018/02/05/neurips-2017-notes.html",
            "relUrl": "/2018/02/05/neurips-2017-notes.html",
            "date": " ‚Ä¢ Feb 5, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "2nd Place Solution Cikm 2017",
            "content": "Second Place Solution at CIKM AnalytiCup 2017 ‚Äì Lazada Product Title Quality Challenge . by Tam Nguyen . Lazada Product Title Quality Challenge . In this challenge, the participants were provided with a set of product titles, description, and attributes, together with the associated title quality scores (clarity and conciseness) as labeled by Lazada internal QC team. The task is to build a product title quality model that can automatically grade the clarity and the conciseness of a product title. . Team members . Tam T. Nguyen, Kaggle Grandmaster, Postdoctoral Research Fellow at Ryerson University | Hossein Fani, PhD Student at University of New Brunswick | Ebrahim Bagheri, Associate Professor at Ryerson University | Gilberto Titericz, Kaggle Grandmaster ( #1 Kaggler), Data Scientist at AirBnb | . Solution Overview . We present our winning approach for the Lazada Product Title Quality Challenge for the CIKM Cup 2017 where the data set was annotated as conciseness and clarity by Lazada QC team. . The participants were asked to build machine learning model to predict conciseness and clarity of an SKU based on product title, short description, product categories, price, country, and product type. As sellers could freely enter anything for title and description, they might contain typos or misspelling words. Moreover, there were many annotators labelling the data so there must be disagreement on the true label of an SKU. This makes the problem difficult to solve if one is solely using traditional natural language processing and machine learning techniques. . In our proposed approach, we adapted text mining and machine learning methods which take into account both feature and label noises. Specifically, we are using bagging methods to deal with label noise where the whole training data cannot be used to build our models. Moreover, we think that for each SKU, conciseness and clarity would be annotated by the same QC. It means that conciseness and clarity should be correlated in a certain manner. Therefore, we extended our bagging approach by considering out of fold leakage to take advantage of co-relation information. . Our proposed approach achieved the root mean squared error (RMSE) of 0.3294 and 0.2417 on the test data for conciseness and clarity, respectively. You may refer to the paper or source code for more details. .",
            "url": "kaggler.com/2017/12/05/2nd-place-solution-cikm-2017.html",
            "relUrl": "/2017/12/05/2nd-place-solution-cikm-2017.html",
            "date": " ‚Ä¢ Dec 5, 2017"
        }
        
    
  
    
        ,"post9": {
            "title": "Winners Solution Porto Seguro",
            "content": "Winner‚Äôs Solution at Porto Seguro‚Äôs Safe Driver Prediction Competition . by Jeong-Yoon Lee . Winner‚Äôs Solution at Porto Seguro‚Äôs Safe Driver Prediction Competition | Feature Engineering | Local Validation | Normalization | Unsupervised Learning | Learning with Train+Test Features Unsupervised | Other Unsupervised Models | Neural Nets | LightGBM | XGBoost | Blending | Software Used | Hardware Used | Total Time Spent | What Did Not Work | The Porto Seguro Safe Driver Prediction competition at Kaggle finished 2 days ago. 5,170 teams with 5,798 people competed for 2 months to predict if a driver will file an insurance claim next year with anonymized data. . Michael Jahrer, Netflix Grand Prize winner and Kaggle Grandmaster, took the lead from the beginning and finished #1. He graciously shared his solution right after the competition. Let‚Äôs check out his secret sauce. (This was initially posted on the Kaggle forum and reposted here with minor format changes with permission from him.) . . Thanks to Porto Seguro to provide us with such a nice, leakage-free, time-free and statistical correct dataset. . A nice playground to test the performance of everything, this competition was stat similar to Otto, like larger testset than train, anonymous data, but differ in details. . I wanna dive straight into solution. . It‚Äôs a blend of 6 models. 1x lightgbm, 5x nn. All on same features, I just removed *calc and added 1-hot on *cat. All neural nets are trained on denoising autoencoder hidden activation, they did a great job in learning a better representation of the numeric data. lightgbm on raw data. Nonlinear stacking failed, simple averaging works best (all weights=1). . That‚Äôs the final 0.2965 solution. 2 single models would have been enough to win (#1 + #2 give me 0.29502 on private). . The complete list of models in the final blend: . . Font is a bit small, you need to increase the zoom with ctrl (+). . The difference to my private .2969 score is I added bagging versions nBag=32 of the above mentioned 6 models, all weight=1, and Igor‚Äôs 287 script with weight=0.05. Was not really worth the effort for .2965 -&gt; .2969 gain huh!? I selected these 2 blends at the end. . Feature Engineering . I dislike this part most, my creativity is too low for an average competition lifetime, also luck plays huge role here. Therefore I like representation learning, its also an step towards AI. . Basically I removed calc, added 1-hot to *cat features. That‚Äôs all I‚Äôve done. No missing value replacement or something. This is feature set ‚Äúf0‚Äù in the table. This ends up in exactly 221 dense features. With single precision floats its 1.3GB RAM (1e-94221(595212+892816)). . Thanks to the public kernels (wheel of fortune eg.) that suggest to remove *calc features, I‚Äôm too blind and probably would not have figured this out by myself. I never remove features. . Local Validation . 5-fold CV as usual. Fixed seed. No stratification. Each model has own rand seed in CV (weight init in nn, data_random_seed in lightgbm). Test predictions are arithmetic averages of all fold models. Just standard as I would use for any other task. Somebody wrote about bagging and its improvements, I spend a week in re-training all my models in a 32-bag setup (sampling with replacement). Score only improved a little. . Normalization . Input normalization for gradient-based models such as neural nets is critical. For lightgbm/xgb it does not matter. The best what I found during the past and works straight of the box is ‚ÄúRankGauss‚Äù. Its based on rank transformation. First step is to assign a linspace to the sorted features from 0..1, then apply the inverse of error function ErfInv to shape them like Gaussian, then I subtract the mean. Binary features are not touched with this transformation (eg. 1-hot ones). This works usually much better than standard mean/std scaler or min/max. . Unsupervised Learning . Denoising autoencoders (DAE) are nice to find a better representation of the numeric data for later neural net supervised learning. One can use train+test features to build the DAE. The larger the testset, the better üôÇ An autoencoder tries to reconstruct the inputs features. So features = targets. Linear output layer. Minimize MSE. A denoising autoencoder tries to reconstruct the noisy version of the features. It tries to find some representation of the data to better reconstruct the clean one. . With modern GPUs we can put much computing power to solve this task by touching peak floating point performance with huge layers. Sometimes I saw over 300W power consumption by checking nvidia-smi. . So why manually constructing 2,3,4-way interactions, use target encoding, search for count features, impute features, when a model can find something similar by itself? . The critical part here is to invent the noise. In tabular datasets we cannot just flip, rotate, sheer like people are doing this in images. Adding Gaussian or uniform additive / multiplicative noise is not optimal since features have different scale or a discrete set of values that some noise just didn‚Äôt make sense. I found a noise schema called ‚Äúswap noise‚Äù. Here I sample from the feature itself with a certain probability ‚ÄúinputSwapNoise‚Äù in the table above. 0.15 means 15% of features replaced by values from another row. . Two different topologies are used by myself. Deep stack, where the new features are the values of the activations on all hidden layers. Second, bottleneck, where one middle layer is used to grab the activations as new dataset. This DAE step usually blows the input dimensionality to 1k..10k range. . Learning with Train+Test Features Unsupervised . You might think I am cheating when using test features too for learning. So I‚Äôve done an experiment to check the effectiveness of unsupervised learning without test features. For reference I took model #2, public:0.28970, private:0.29298. With exactly same params it ends up in a slightly weaker CV gini:0.2890. public:0.28508, private:0.29235. Private score is similar, public score is worse. So not a complete breakdown as expected. Btw total scoring time of the testset with this ‚Äúclean‚Äù model is 80[s]. . Other Unsupervised Models . Yes I tried GANs (generative adversarial networks) here. No success. Since NIPS2016 I was able to code GANs by myself. A brilliant idea. Generated MNIST digits looked fine, CIFAR images not that. . For generator and discriminator I used MLPs. I think they have a fundamental problem in generating both numeric and categorical data. The discriminator won nearly all the time on my setups. I tried various tricks like truncation the generator output. Clip to known values, many architectures, learn params, noise vec length, dropout, leakyRelu etc. Basically I used activations from hidden layers of the discrimiator as new dataset. At the end they were low 0.28x on CV, too low to contribute to the blend. Haven‚Äôt tried hard enough. . Another idea that come late in my mind was a min/max. game like in GAN to generate good noise samples. Its critical to generate good noise for a DAE. I‚Äôm thinking of a generator with feature+noiseVec as input, it maximizes the distance to original sample while the autoencoder (input from generator) tried to reconstruct the sample‚Ä¶ more maybe in another competition. . Neural Nets . Feedforward nets trained with backprop, accelerated by minibatch gradient updates. This is what all do here. I use vanilla SGD (no momentum or adam), large number of epochs, learning rate decay after every epoch. Hidden layers have ‚Äòr‚Äô = relu activation, output is sigmoid. Trained to minimize logloss. In bottleneck autoencoder the middle layer activation is ‚Äòl‚Äô = linear. When dropout!=0 it means all hidden layers have dropout. Input dropout often improve generalization when training on DAE features. Here a slight L2 regularization also helps in CV. Hidden layer size of 1000 works out of the box for most supervised tasks. All trained on GPU with 4-byte floats. . LightGBM . Nice library, very fast, sometimes better than xgboost in terms of accuracy. One model in the ensemble. I tuned params on CV. . XGBoost . I didn‚Äôt found a setup where xgboost adds something to the blend. So no used here in Porto. . Blending . Nonlinear things failed. That‚Äôs the biggest difference to the Otto competition where xgb, nn were great stackers. Every competition has its own pitfalls. Whatever. For me even tuning of linear blending weights failed. So I stick with all w=1. . Software Used . Everything I‚Äôve done here end-to-end was written in C++/CUDA by myself. Of course I used lightgbm and xgboost C interface and a couple of acceleration libs like cuBLAS. I‚Äôm a n00b in python or R like you guys are experts. My approach is still old school and low level. I want to understand what is going from top to bottom. At some time, I‚Äôll learn it, but currently there are just too much python/R packages that bust my head, I‚Äôm stick with loop-based code. . Hardware Used . All models above can be run on a 32GB RAM machine with clever data swapping. Next to that I use a GTX 1080 Ti card for all neural net stuff. . Total Time Spent . Some exaflops and kilowatts of GPU power was wasted for this competition for sure. Models run longer than I spend on writing code. Reading all the forum posts also costs a remarkable amount of time, but here my intention was don‚Äôt miss anything. At the end it was all worth. Big hands to all the great writers here like Tilli, CPMP, .. really great job guys. . What Did Not Work . Upsampling, deeper autoencoders, wider autoencoders, KNNs, KNN on DAE features, nonlinear stacking, some feature engineering (yes, I tried this too), PCA, bagging, factor models (but others had success with it), xgboost (other did well with that) and much much more.. . That‚Äôs it. .",
            "url": "kaggler.com/2017/12/01/winners-solution-porto-seguro.html",
            "relUrl": "/2017/12/01/winners-solution-porto-seguro.html",
            "date": " ‚Ä¢ Dec 1, 2017"
        }
        
    
  
    
        ,"post10": {
            "title": "Quora How Many Employed Ds",
            "content": "Quora: How many employed data scientists are able to solve problems from online competitions such as Kaggle‚Äôs? . by Jeong-Yoon Lee . Before reading further, please watch this video (only 1m 47s long), which shows how an average man compares to a football player at 40 yard dash. . When I talk to data science professionals, especially senior ones with more experience, I often encounter optimism on one‚Äôs competitiveness - ‚ÄúI know what I am doing and can build good models at work - maybe better than others‚Äù. . Online competitions provide objective measures for at least a few criteria, such as prediction accuracy, time to build a good model, reproducibility, etc. . For most data scientists, including myself, working on competitions is a reality check and humbling experience: . At the Intelligence Advanced Research Projects Activity (IARPA) tournament, the performance of ‚Äúsuperforecasters‚Äù was 50% better than other forecasters, and 30% better than even those with access to secret data 1. | At KDD Cup 2015, the winning teams achieved over 90% accuracy while over 100 teams remained around 60% accuracy, 30% lower than the best score 2. | At Criteo Display Advertising Challenge, the benchmark solution provided by a well respected domain expert was outperformed by a simple 100+ lines of Python code written by a Kaggle user, tinrtgu. | . Long tenure doesn‚Äôt guarantee superior performance. As summarized by Dr. Ericsson in his bestseller book, Peak, the doctor, teacher, or driver with twenty years of experiences is likely to be worse than the one with only five because one‚Äôs performance deteriorates gradually with years of routine/automated work in the absence of deliberate efforts to improve. . Going back to the original question, employed data scientists ‚Äúwithout learning from competitions‚Äù are likely to do very poorly on competitions. . The learning doesn‚Äôt need to come from participating in competitions. Out of 1MM+ Kaggle users, only 65K+ participate in competitions, while others learn cutting edge algorithms and best practices from tutorials, solutions shared by others, working on open data sets, etc. . Whenever I talk to someone who discounts the benefits of competitions without having a single competition experience, and yet is very confident on her/his modeling capability, I can‚Äôt help thinking about the average-man-vs-football-player video above, and just smile. :) . Competing against 0.1% improvement in accuracy? It‚Äôs like criticizing that Olympian 100m sprinters compete for 0.1 sec. That‚Äôs not for most of us. Don‚Äôt worry about it until you get close. We have much longer way to go. . Footnotes . Superforecasting: The Art and Science of Prediction eBook: Philip E. Tetlock, Dan Gardner: Kindle Store¬†&#8617; . | Rank¬†&#8617; . |",
            "url": "kaggler.com/2017/08/28/quora-how-many-employed-ds.html",
            "relUrl": "/2017/08/28/quora-how-many-employed-ds.html",
            "date": " ‚Ä¢ Aug 28, 2017"
        }
        
    
  
    
        ,"post11": {
            "title": "New Editor Tam",
            "content": "New Editor ‚Äì Tam T. Nguyen, Kaggle Grandmaster . by Jeong-Yoon Lee . . I am happy to announce that we have a new editor, Tam T. Nguyen, joining Kaggler.com. . Tam is a Competition Grandmaster at Kaggle. He won the 1st prizes at KDD Cup 2015, IJCAI-15 repeat buyer competition, and Springleaf marketing response competition. . Currently, he is Postdoctoral Search Fellow at Ryerson University in Toronto, Canada. Prior to that, he was Data Analytics Project Lead at I2R A*STAR. He earned his Ph.D. in Computer Science from NTU Singapore. He‚Äôs originally from Vietnam. . Please subscribe to us at Kaggler.com, Facebook, and Twitter. .",
            "url": "kaggler.com/2017/07/15/new-editor-tam.html",
            "relUrl": "/2017/07/15/new-editor-tam.html",
            "date": " ‚Ä¢ Jul 15, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Keras Backend Benchmark",
            "content": "Keras Backend Benchmark: Theano vs TensorFlow vs CNTK . by Jeong-Yoon Lee . Inspired by Max Woolf‚Äôs benchmark, the performance of 3 different backends (Theano, TensorFlow, and CNTK) of Keras with 4 different GPUs (K80, M60, Titan X, and 1080 Ti) across various neural network tasks are compared. . For the performance of TensorFlow and CNTK with K80, the numbers reported at Max Woolf‚Äôs benchmark are used. . Conclusion . The accuracies of Theano, TensorFlow and CNTK backends are similar across all benchmark tests, while speeds vary a lot. . Theano is significantly (up to 50 times) slower than TensorFlow and CNTK. | Between TensorFlow and CNTK, CNTK is a lot (about 2 to 4 times) faster than TensorFlow for LSTM (Bidirectional LSTM on IMDb Data and Text Generation via LSTM), while speeds for other type of neural networks are close to each other. | . Among K80, M60, Titan X and 1080 Ti GPUs: . 1080 Ti is the fastest. | K80 is the slowest. | M60 is faster than K80 and comparable to Titan X and 1080 Ti. | Theano is significantly (up to 14 times) faster on 1080 Ti than on Titan X, while the improvements for TensorFlow and CNTK are moderate. | . Detailed results are available at https://github.com/szilard/benchm-dl/blob/master/keras_backend.md .",
            "url": "kaggler.com/2017/07/12/keras-backend-benchmark.html",
            "relUrl": "/2017/07/12/keras-backend-benchmark.html",
            "date": " ‚Ä¢ Jul 12, 2017"
        }
        
    
  
    
        ,"post13": {
            "title": "Quora How To Become Kaggle Master",
            "content": "Quora: How did you become a Kaggle Master . by Jeong-Yoon Lee . Thanks Hui Jun Ng for A2A. . I can share about how to become a Kaggle Competition Master. There are 2 other kinds of Kaggle Masters: the Kernel Master and Discussion Master 1. . I wrote Jeong-Yoon Lee‚Äôs answer to What does it take to rank within #10 on Kaggle? What is an ideal learning path for a beginner in data science? What all technologies and skills does one need to acquire and in what order? How long does it take? before and it is, not surprisingly, relevant to this question as well. . At the time of writing (7/9/2017), there are about 1,000 Kagglers who are either competition Grand Masters (92) or Masters (868). . . The requirements to become a Kaggle Competition Master are: . Gold Medal: top 10 in 1 competition | Silver Medals: top 5% in 2 competitions | . Earning 2 silver medals is something that you can achieve eventually just by practicing at Kaggle consistently. As you learn from best practices shared by others on Kernels and forums at each competition, you will get closer to the top. . However, earning 1 gold medal is different. Even many (26) of top 100 Kagglers have only 1 or 2 gold medals. It requires good data science skills such as ETL, EDA, modeling, validation, and ensembles as well as other things such as creative post processing, hacks, good team work and lots of luck. :) . . To share my story, honestly I don‚Äôt remember how and when I became a Kaggle Master because I didn‚Äôt compete for it. . When I started Kaggle 6 years ago, I was fascinated by the idea of competing against other data scientists across the world. Also I was amazed by top Kagglers on how fast they came up with great solutions at ANY competitions. . I just wanted to learn from them to become a better data scientist. So I kept participating in competitions, talking to them, teaming up with them, and learning from them. . Let me rephrase what I said at Jeong-Yoon Lee‚Äôs answer to What does it take to rank within #10 on Kaggle? What is an ideal learning path for a beginner in data science? What all technologies and skills does one need to acquire and in what order? How long does it take? here: . I never thought that I would be the top 10 because there were so many Kagglers who were (and still are) much better than myself. I just enjoyed competing at Kaggle, worked on competitions regularly, teamed up with great people, and was really lucky. . If you enjoy the journey itself, whether you make the top 10 or not doesn‚Äôt really matter, but at the same time, if the chance comes, I hope you will be there ready to take it. . Enjoy! . Footnotes . Kaggle Progression System¬†&#8617; . |",
            "url": "kaggler.com/2017/07/10/quora-how-to-become-kaggle-master.html",
            "relUrl": "/2017/07/10/quora-how-to-become-kaggle-master.html",
            "date": " ‚Ä¢ Jul 10, 2017"
        }
        
    
  
    
        ,"post14": {
            "title": "Quora What Does It Feel Like",
            "content": "Quora: What does it feel like to be addicted to Kaggle? . by Jeong-Yoon Lee . Giuliano Janson gave a great perspective as an ex-Kaggle addict who successfully recovered from it. Let me give mine as an active Kaggle addict, who is helpless without any hope for rehab. ;) . I have a weekly Data Science journal club at work, which I turned into a weekly Kaggle club. I invite people from outside the company as well and now we have about 10 people joining from 4 different companies. . Today, as we did post-mortem on the Quora competiton that ended earlier this week, one of attendees, who is relatively new to Kaggle, shared that she found it very addictive. . I agreed with her and added that it is addictive because it is so to observe yourself improving, just like how work-out and career advancement can be addictive. It is a different kind of addiction from ones that harm yourself. . At the post-mortem meeting, we shared lists of things that we learned from the competition: Feature engineering in NLP with various embeddings, good ML implementations such as LightGBM, XGBoost and Keras, new deep learning architectures combining LSTM or CNN with meta features, and many more. . And we talked about which competition to enter next. Yeah, we don‚Äôt need a break. We need another dose of a competiton, or , as I like to put, a deliberate practice opportunity. . One tip for me to remain addicted for a long time - past 6 years - is that I don‚Äôt compete to rank higher, but I compete to learn more and have fun with others. That‚Äôs why I compete with a team with different people at various skill levels, including first time Kagglers. . How do I feel? I feel great to be addicted to Kaggle and even better to be addicted to self improvement. :) . At the Quora competition, we finished 36th out of 3,307 teams. One of members was a first timer. Yeah, I gave him a pretty good gateway drug. No regret! ;) .",
            "url": "kaggler.com/2017/06/12/quora-what-does-it-feel-like.html",
            "relUrl": "/2017/06/12/quora-what-does-it-feel-like.html",
            "date": " ‚Ä¢ Jun 12, 2017"
        }
        
    
  
    
        ,"post15": {
            "title": "Build Kaggle Machine",
            "content": "Building Your Own Kaggle Machine . . In 2014, I shared the specifications of a 6-core 64GB RAM desktop system that I purchased at around USD 2,000. Since then, I added NVidia Titan X to it for deep learning at additional USD 1,000, and it served me well. . However, as other team members started joining me on data science competitions and deep learning competitions got more popular, my team decided to build a more powerful desktop system. . The specifications of the new system that we built are as follows: . CPU: Xeon 2.4GHz 14-Core | RAM: 128GB DDR4-2400 | GPU: 4 NVidia 1080 Ti 11GB | SSD: 960GB | HDD: 4TB 7200RPM | PSU: 1600W 80+ Titanium certified | . Total cost including tax and shipping was around USD 7,000. Depending on the budget, you can go down to 2 (-USD 1,520) 1080 Ti GPU cards instead of 4, or 64GB (-USD 399) instead of 128GB RAM, and still have a decent system at around USD 5,000. . You can find the full part lists here. . Additional Resources . Which GPU(s) to Get for Deep Learning by Tim Dettmers | Building a 32-Thread Xeon Monster PC for Less Than the Price of a Flagship Core i7 by TechSpot | .",
            "url": "kaggler.com/2017/04/06/build-kaggle-machine.html",
            "relUrl": "/2017/04/06/build-kaggle-machine.html",
            "date": " ‚Ä¢ Apr 6, 2017"
        }
        
    
  
    
        ,"post16": {
            "title": "Winning Data Science Competitions",
            "content": "Winning Data Science Competitions ‚Äì Latest Slides . . This year I had several occasions to give my ‚ÄúWinning Data Science Competitions‚Äù talk ‚Äì at Microsoft, KSEA-SWC 2017, USC Applied Statistics Club, Spark SC, and Whisper. . I am grateful for all these opportunities to share what I enjoy with the data scientist community. . I truly believe that working on competitions on a regular basis can make us better data scientists. Hope my talk and slides help other data scientists. . My talk is outlined as follows: . Why compete For fun | For experience | For learning | For networking | . | Data science competition intro Competitions | Structure | Kaggle | . | Misconceptions of data science competitions No ETL? | No EDA? | Not worth it? | Not for production? | . | Best practices Feature engineering | Diverse algorithms | Cross validation | Ensemble | Collaboration | . | Personal tips | Additional resources | . You can find latest slides here .",
            "url": "kaggler.com/2017/04/05/winning-data-science-competitions.html",
            "relUrl": "/2017/04/05/winning-data-science-competitions.html",
            "date": " ‚Ä¢ Apr 5, 2017"
        }
        
    
  
    
        ,"post17": {
            "title": "Kaggler 0 5 Released",
            "content": "Kaggler 0.5.0 Released . by Jeong-Yoon Lee . I am glad to announce the release of Kaggler 0.5.0. Kaggler 0.5.0 has a significant improvement in the performance of the FTRL algorithm thanks to Po-Hsien Chu (github, kaggle, linkedin). . Results . We increase the train speed by up to 100 times compare to 0.4.x. Our benchmark shows that one epoch with 1MM records with 8 features takes 1.2 seconds with 0.5.0 compared to 98 seconds with 0.4.x on an i7 CPU. . Motivation . The FTRL algorithm has been a popular algorithm since its first appearance on a paper published by Google. It is suitable for highly sparse data, so it has been widely used for click-through-rate (CTR) prediction in online advertisement. Many Kagglers use FTRL as one of their base algorithms in CTR prediction competitions. Therefore, we want to improve our FTRL implementation and benefit Kagglers who use our package. . Methods . We profile the code with cProfile and resolve the overheads one by one: . Remove over-heads of Scipy Sparse Matrix row operation: Scipy sparse matrix checks many conditions in __getitems__, resulting in a lot of function calls. In fit(), we know that we‚Äôre fetching exactly each row, and it is very unlikely to exceed the bound, so we can fetch the indexes of each row in a faster way. This enhancement makes our FTRL 10x faster. | More c-style enhancement: Specify types more clearly, return a whole list instead of yielding feature indexes, etc. These enhancements make our FTRL 5X faster when interaction==False. | Faster hash function for interaction features: The last enhancement is to remove the overhead of hashing of interaction features. We use MurMurHash3, which scikit-learn uses, to directly hash the multiplication of feature indexes. This enhancement makes our FTRL 5x faster when interaction==True. | . Contributor . Po-Hsien Chu (github, kaggle, linkedin) .",
            "url": "kaggler.com/2017/01/11/kaggler-0-5-released.html",
            "relUrl": "/2017/01/11/kaggler-0-5-released.html",
            "date": " ‚Ä¢ Jan 11, 2017"
        }
        
    
  
    
        ,"post18": {
            "title": "Ds Packages",
            "content": "Great Packages for Data Science in Python and R . by Hang Li . Domino‚Äôs Chief Data Scientist, Eduardo Ari√±o de la Rubia talk about Python and R as the ‚Äúbest‚Äù language for data scientists. A list of useful packages from this talk. . Python . Feather ‚Äì Fast, interoperable binary data frame storage for Python, R, and more powered by Apache Arrow | Ibis ‚Äì Productivity-centric Python data analysis framework for SQL systems and the Hadoop platform. Co-founded by the creator of pandas | Paratext ‚Äì A library for reading text files over multiple cores. | Bcolz ‚Äì A columnar data container that can be compressed. | Altair ‚Äì Declarative statistical visualization library for Python | Bokeh ‚Äì Interactive Web Plotting for Python | Blaze ‚Äì NumPy and Pandas interface to Big Data | Xarry ‚Äì N-D labeled arrays and datasets in Python | Dask ‚Äì Versatile parallel programming with task scheduling | Keras ‚Äì High-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. | PyMC3 ‚Äì Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano | . R . Feather ‚Äì Fast, interoperable binary data frame storage for Python, R, and more powered by Apache Arrow | Haven ‚Äì Import foreign statistical formats into R via the embedded ReadStat C library. | readr ‚Äì Read flat/tabular text files from disk (or a connection). | Jsonlite A fast JSON parser and generator optimized for statistical data and the web. | ggplot2 ‚Äì A system for ‚Äòdeclaratively‚Äô creating graphics, based on ‚ÄúThe Grammar of Graphics‚Äù. | htmlwidgets ‚Äì A framework for creating HTML widgets that render in various contexts including the R console, ‚ÄòR Markdown‚Äô documents, and ‚ÄòShiny‚Äô web applications. | leaflet ‚Äì Create and customize interactive maps using the ‚ÄòLeaflet‚Äô JavaScript library and the ‚Äòhtmlwidgets‚Äô package. | tilegramsR ‚Äì Provide R spatial objects representing Tilegrams. | dplyr ‚Äì A fast, consistent tool for working with data frame like objects, both in memory and out of memory. | broom ‚Äì Convert statistical analysis objects from R into tidy data frames | tidytext ‚Äì Text mining for word processing and sentiment analysis using ‚Äòdplyr‚Äô, ‚Äòggplot2‚Äô, and other tidy tools. | mxnet ‚Äì The MXNet R packages brings flexible and efficient GPU computing and state-of-art deep learning to R. | tensorflow ‚Äì TensorFlow‚Ñ¢ is an open source software library for numerical computation using data flow graphs. | .",
            "url": "kaggler.com/2016/12/30/ds-packages.html",
            "relUrl": "/2016/12/30/ds-packages.html",
            "date": " ‚Ä¢ Dec 30, 2016"
        }
        
    
  
    
        ,"post19": {
            "title": "Allstate Competition",
            "content": "Solution Sharing for the Allstate Competition at Kaggle . I participated in the Allstate competition at Kaggle and finished 54th out of 3,055 teams. I shared my solution in the forum after the competition here: . . Congrats for winners and top performers, and thanks for great sharing to all contributors in the forum. It‚Äôs always a humbling experience to compete at Kaggle. I learn so much at every competition from a lot of fellow kagglers. . Here I‚Äôd like to share my code base and notes for the competition: . Code repo with competition notes | Wiki page with internal LB | . My friends and I have been using the framework based on Makefiles for competitions for years now and it has worked great so far. . Introduction to the framework is available on the TalkingData forum . Our previous code repo for past competitions are also available at: . For Bosch (#22) | For TalkingData (#37) | . Hope it‚Äôs helpful. .",
            "url": "kaggler.com/2016/12/13/allstate-competition.html",
            "relUrl": "/2016/12/13/allstate-competition.html",
            "date": " ‚Ä¢ Dec 13, 2016"
        }
        
    
  
    
        ,"post20": {
            "title": "Predictive Modeling",
            "content": "Predictive Modeling: Why the ‚Äúwho‚Äù is just as important as the ‚Äúhow‚Äù . by Jeong-Yoon Lee . There is significant debate in the data science community around the most important ingredients for attaining accurate results from predictive models. Some claim that it‚Äôs all about the quality and/or quantity of data, that you need a certain size data set (typically large) of a particular quality (typically very good) in order to get meaningful outputs. Others focus more on the models themselves, debating the merits of different single models ‚Äì deep learning, gradient boosting machine, Gaussian process, etc. ‚Äì versus a combined approach like the Ensemble Method. . I think that both of these positions have some truth. While it‚Äôs not as simple as ‚Äúmore data, better results‚Äù (see cases from Twitter and Netflix showing that the volume of data was almost meaningless to predictive accuracy), nor is the model itself the only predictor of success, all of those elements do play a role in how precise the results will be. But there is another factor that is almost always overlooked: the modelers themselves. Like the models they create, not all data scientists are created equal. I am less interested in who is ‚Äúsmarter‚Äù or has a better education, and more in how competitive and dedicated the modeler is. Most marketers don‚Äôt question the qualifications of a data science team because they expect that given good data and a solid algorithmic approach, they will achieve good predictive performance. At the very least, performance across different modelers should be comparable. Unfortunately, that‚Äôs not always the case. . In his New York Times bestseller Superforecasting, Prof. Philip Tetlock at University of Pennsylvania showed that, at the Intelligence Advanced Research Projects Activity (IARPA) tournament, the performance of ‚Äúsuperforecasters‚Äù was 50% better than standard, and 30% better than those with access to secret data. This clearly demonstrates that the people doing the modeling, not the data or the models themselves, make a huge difference. . More relevant to predictive modeling specifically, KDD, one of most prestigious data science conferences, has hosted an annual predictive modeling competition, KDD Cup, since 1997. It attracts participants from top universities, companies, and industries around the world. Although every team is given exactly the same data set, and is familiar with same state-of-the-art algorithms, the resulting performances vary wildly across teams. Last year, the winning team achieved 91% accuracy while over 100 teams remained below 63% accuracy, 30% lower than the best score. . Both of these examples show the importance of not just the ‚Äúhow,‚Äù but the ‚Äúwho‚Äù when it comes to predictive modeling. This isn‚Äôt always the easiest thing for marketers to assess, but should definitely be taken into consideration when evaluating predictive analytics solutions. Ask about the data, and the models and methodology, but don‚Äôt forget the modelers themselves. The right data scientists can make all the difference to the success of your predictive program. .",
            "url": "kaggler.com/2016/08/06/predictive-modeling.html",
            "relUrl": "/2016/08/06/predictive-modeling.html",
            "date": " ‚Ä¢ Aug 6, 2016"
        }
        
    
  
    
        ,"post21": {
            "title": "Winning Data Science Competitions",
            "content": "Winning Data Science Competitions @ datascience.la . . On October 27th, I presented my favorite topic, Data Science competitions, at the DataScience.LA meetup. . Here are the slides and video. . Slides | Video | .",
            "url": "kaggler.com/2015/11/19/winning-data-science-competitions.html",
            "relUrl": "/2015/11/19/winning-data-science-competitions.html",
            "date": " ‚Ä¢ Nov 19, 2015"
        }
        
    
  
    
        ,"post22": {
            "title": "Kagglers Toolbox",
            "content": "Kaggler‚Äôs Toolbox ‚Äì Setup . . I‚Äôd like to open up my toolbox that I‚Äôve built for data mining competitions, and share with you. . Let me start with my setup. . Kaggler‚Äôs Toolbox ‚Äì Setup | System | Git | S3 / Dropbox | Makefile | SSH Tunneling | tmux | System . I have access to 2 machines: . Laptop ‚Äì Macbook Pro Retina 15‚Ä≥, OS X Yosemite, i7 2.3GHz 4 Core CPU, 16GB RAM, GeForce GT 750M 2GB, 500GB SSD | Desktop ‚Äì Ubuntu 14.04, i7 5820K 3.3GHz 6 Core CPU, 64GB RAM, GeForce GT 620 1GB, 120GB SSD + 3TB HDD | . I purchased the desktop from eBay around at $2,000 a year ago (September 2014). . Git . As the code repository and version control system, I use git. . It‚Äôs useful for collaboration with other team members. It makes easy to share the code base, keep track of changes and resolve conflicts when two people change the same code. . It‚Äôs useful even when I work by myself too. It helps me reuse and improve the code from previous competitions I participated in before. . For competitions, I use gitlab instead of github because it offers unlimited number of private repositories. . S3 / Dropbox . I use S3 to share files between my machines. It is cheap ‚Äì it costs me about $0.1 per month on average. . To access S3, I use AWS CLI. I also used to use s3cmd and like it. . I use Dropbox to share files between team members. . Makefile . For flow control or pipelining, I use makefiles (or GNU make). . It modularizes the long process of a data mining competition into feature extraction, single model training, and ensemble model training, and controls workflow between components. . For example, I have a top level makefile that defines the raw data file locations, folder hierarchies, and target variable. . Makefile . # directories DIR_DATA := data DIR_BUILD := build DIR_FEATURE := $(DIR_BUILD)/feature DIR_METRIC := $(DIR_BUILD)/metric DIR_MODEL := $(DIR_BUILD)/model # directories for the cross validation and ensembling DIR_VAL := $(DIR_BUILD)/val DIR_TST := $(DIR_BUILD)/tst DIRS := $(DIR_DATA) $(DIR_BUILD) $(DIR_FEATURE) $(DIR_METRIC) $(DIR_MODEL) $(DIR_VAL) $(DIR_TST) # data files for training and predict DATA_TRN := $(DIR_DATA)/train.csv DATA_TST := $(DIR_DATA)/test.csv SAMPLE_SUBMISSION := $(DIR_DATA)/sample_submission.csv ID_TST := $(DIR_DATA)/id.tst.csv HEADER := $(DIR_DATA)/header.csv CV_ID := $(DIR_DATA)/cv_id.txt Y_TRN:= $(DIR_FEATURE)/y.trn.txt Y_TST:= $(DIR_FEATURE)/y.tst.txt $(DIRS): mkdir -p $@ $(HEADER): $(SAMPLE_SUBMISSION) head -1 $&lt; &gt; $@ $(ID_TST): $(SAMPLE_SUBMISSION) cut -d, -f1 $&lt; | tail -n +2 &gt; $@ $(Y_TST): $(SAMPLE_SUBMISSION) | $(DIR_FEATURE) cut -d, -f2 $&lt; | tail -n +2 &gt; $@ $(Y_TRN) $(CV_ID): $(DATA_TRN) | $(DIR_FEATURE) python src/extract_target_cvid.py --train-file $&lt; --target-file $(Y_TRN) --cvid-file $(CV_ID) # cleanup clean:: find . -name &#39;*.pyc&#39; -delete clobber: clean -rm -rf $(DIR_DATA) $(DIR_BUILD) .PHONY: clean clobber mac.setup ubuntu.setup apt.setup pip.setup . Then, I have makefiles for features that includes the top level makefile, and defines how to generate training and test feature files in various formats (CSV, libSVM, VW, libFFM, etc.). . Makefile.feature.j3 . #-- # j3: h2 + row id #-- include Makefile FEATURE_NAME := j3 FEATURE_TRN := $(DIR_FEATURE)/$(FEATURE_NAME).trn.sps FEATURE_TST := $(DIR_FEATURE)/$(FEATURE_NAME).tst.sps $(FEATURE_TRN) $(FEATURE_TST): $(DATA_TRN) $(DATA_TST) | $(DIR_FEATURE) python ./src/generate_$(FEATURE_NAME).py --train-file $&lt; --test-file $(word 2, $^) --train-feature-file $(FEATURE_TRN) --test-feature-file $(FEATURE_TST) . Then, I have makefiles for single model training that includes a feature makefile, and defines how to train a single model and produce CV and test predictions. . Makefile.xg . include Makefile.feature.j3 N = 10000 DEPTH = 6 LRATE = 0.05 SUBCOL = 1 SUBROW = 0.8 SUBLEV = 0.5 WEIGHT = 1 N_STOP = 100 ALGO_NAME := xg_$(N)_$(DEPTH)_$(LRATE)_$(SUBCOL)_$(SUBROW)_$(SUBLEV)_$(WEIGHT)_$(N_STOP) MODEL_NAME := $(ALGO_NAME)_$(FEATURE_NAME) METRIC_VAL := $(DIR_METRIC)/$(MODEL_NAME).val.txt PREDICT_VAL := $(DIR_VAL)/$(MODEL_NAME).val.yht PREDICT_TST := $(DIR_TST)/$(MODEL_NAME).tst.yht SUBMISSION_TST := $(DIR_TST)/$(MODEL_NAME).sub.csv SUBMISSION_TST_GZ := $(DIR_TST)/$(MODEL_NAME).sub.csv.gz all: validation submission validation: $(METRIC_VAL) submission: $(SUBMISSION_TST) retrain: clean_$(ALGO_NAME) submission $(PREDICT_TST) $(PREDICT_VAL): $(FEATURE_TRN) $(FEATURE_TST) $(CV_ID) | $(DIR_VAL) $(DIR_TST) ./src/train_predict_xg.py --train-file $&lt; --test-file $(word 2, $^) --predict-valid-file $(PREDICT_VAL) --predict-test-file $(PREDICT_TST) --depth $(DEPTH) --lrate $(LRATE) --n-est $(N) --subcol $(SUBCOL) --subrow $(SUBROW) --sublev $(SUBLEV) --weight $(WEIGHT) --early-stop $(N_STOP) --cv-id $(lastword $^) $(SUBMISSION_TST_GZ): $(SUBMISSION_TST) gzip $&lt; $(SUBMISSION_TST): $(PREDICT_TST) $(HEADER) $(ID_TST) | $(DIR_TST) paste -d, $(lastword $^) $&lt; &gt; $@.tmp cat $(word 2, $^) $@.tmp &gt; $@ rm $@.tmp $(METRIC_VAL): $(PREDICT_VAL) $(Y_TRN) | $(DIR_METRIC) python ./src/evaluate.py --predict-file $&lt; --target-file $(word 2, $^) &gt; $@ cat $@ clean:: clean_$(ALGO_NAME) clean_$(ALGO_NAME): -rm $(METRIC_VAL) $(PREDICT_VAL) $(PREDICT_TST) $(SUBMISSION_TST) find . -name &#39;*.pyc&#39; -delete .DEFAULT_GOAL := all . Then, I have makefiles for ensemble features that defines which single model predictions to be included for ensemble training. . Makefile.feature.esb3 . include Makefile FEATURE_NAME := esb4 BASE_MODELS := xg_10000_6_0.05_1_0.8_0.5_1_100_h2 xg_10000_6_0.05_1_0.8_0.5_1_100_j3 keras_100_2_128_0.5_512_5_h2 keras_100_2_128_0.5_512_5_j3 PREDICTS_TRN := $(foreach m, $(BASE_MODELS), $(DIR_VAL)/$(m).val.yht) PREDICTS_TST := $(foreach m, $(BASE_MODELS), $(DIR_TST)/$(m).tst.yht) FEATURE_TRN := $(DIR_FEATURE)/$(FEATURE_NAME).trn.csv FEATURE_TST := $(DIR_FEATURE)/$(FEATURE_NAME).tst.csv %.sps: %.csv python src/csv_to_sps.py --csv-file $&lt; --sps-file $@ $(FEATURE_TRN): $(Y_TRN) $(PREDICTS_TRN) | $(DIR_FEATURE) paste -d, $^ | tr -d &#39; r&#39; &gt; $@ $(FEATURE_TST): $(Y_TST) $(PREDICTS_TST) | $(DIR_FEATURE) paste -d, $^ | tr -d &#39; r&#39; &gt; $@ clean:: clean_$(FEATURE_NAME) clean_$(FEATURE_NAME): -rm $(FEATURE_TRN) $(FEATURE_TST) . Finally, I can (re)produce the submission from XGBoost ensemble with 9 single models described in Makefile.feature.esb4 by (1) replacing include Makefile.feature.j3 in Makefile.xg with include Makefile.feature.esb4 and (2) running: . $ make -f Makefile.xg . SSH Tunneling . When I‚Äôm connected to Internet, I always ssh to the desktop for its computational resources (mainly for RAM). . I followed Julian Simioni‚Äôs tutorial to allow remote SSH connection to the desktop. It needs an additional system with a publicly accessible IP address. You can setup an AWS micro (or free tier) EC2 instance for it. . tmux . tmux allows you to keep your SSH sessions even when you get disconnected. It also let you split/add terminal screens in various ways and switch easily between those. . Documentation might look overwhelming, but all you need are: . # If there is no tmux session: $ tmux . or . # If you created a tmux session, and want to connect to it: $ tmux attach . Then to create a new pane/window and navigate in between: . Ctrl + b + ‚Äú ‚Äì to split the current window horizontally. | Ctrl + b + % ‚Äì to split the current window vertically. | Ctrl + b + o ‚Äì to move to next pane in the current window. | Ctrl + b + c ‚Äì to create a new window. | Ctrl + b + n ‚Äì to move to next window. | . To close a pane/window, just type exit in the pane/window. . Hope this helps. . Next up is about machine learning tools I use. . Please share your setups and thoughts too. üôÇ .",
            "url": "kaggler.com/2015/09/21/kagglers-toolbox.html",
            "relUrl": "/2015/09/21/kagglers-toolbox.html",
            "date": " ‚Ä¢ Sep 21, 2015"
        }
        
    
  
    
        ,"post23": {
            "title": "Deloitte Competition",
            "content": "60 Day Journey of Deloitte Churn Prediction Competition . . 60 Day Journey of Deloitte Churn Prediction Competition | Competition | Result | Visualization | Closing | Competition . Last December, I teamed up with Michael once again to participate in the Deloitte Churn Prediction competition at Kaggle, where to predict which customers will leave an insurance company in the next 12 months. . It was a master competition, which is open to only master level Kagglers (top 0.2% out of 138K competitors), with $70,000 cash prizes for top 3 finishers. . Result . We managed to do well and finished in 4th place out of 37 teams in spite of that we did not have much time due to projects at work and family events (especially for Michael, who became a dad during the competition). . Although we were little short to earn the prize, it was a fun experience working together with Michael, competing with other top competitors across the world, and climbing the leaderboard day by day. . Visualization . I visualized our 60 day journey during the competition below, and here are some highlights (for us): . Day 22-35: Dived into the competition, set up the github repo and S3 for collaboration, and climbed up the leaderboard quickly. | Day 41-45: Second spurt. Dug in GBM and NN models. Michael‚Äôs baby girl was born on Day 48. | Day 53-60: Last spurt. Ensembled all models. Improved our score every day, but didn‚Äôt have time to train the best models. | . Motion Chart - Deloitte Churn Prediction Leaderboard . Once clicked the link above, it will show a motion chart where: . X-axis: Competition day. From day 0 to day 60. | Y-axis: AUC score. | Colored circle: Each team. If clicked, it shows which team it represents. | Right most legend: Competition day. You can drag up and down the number to see the chart on a specific day. | Initial positions of circles show the scores of their first submissions. | . For the chart, I reused the code using rCharts published by Tony Hirst at github: https://github.com/psychemedia (He also wrote a tutorial on his blog about creating a motion chart using rCharts). . Closing . We took a rain check on this, but will win next time! üôÇ .",
            "url": "kaggler.com/2014/01/03/deloitte-competition.html",
            "relUrl": "/2014/01/03/deloitte-competition.html",
            "date": " ‚Ä¢ Jan 3, 2014"
        }
        
    
  
    
        ,"post24": {
            "title": "Ds Career For Neuroscientist",
            "content": "Data Science Career for Neuroscientists + Tips for Kaggle Competitions . . Recently Prof. Konrad Koerding at Northwestern University asked for an advice on his Facebook for one of his Ph.D student, who studies Computational Neuroscience but wants to pursue his career in Data Science. It reminded me of the time I was looking for such opportunities, and shared my thoughts (now posted on the webpage of his lab here). I decide to post it here with a few fixes so that it can help others. . Data Science Career for Neuroscientists + Tips for Kaggle Competitions | Introduction | Tools in Data Science | Hints for Kaggle Data Mining Competitions Don‚Äôt jump into algorithms too fast. | Try different algorithms and blend. | Optimize at last. | | ‚Äì‚Äî . Introduction . First, I‚Äôd like to say that Data Science is a relatively new field (like Computational Neuroscience), and you don‚Äôt need to feel bad to make the transition after your Ph.D. When I was out to the job market, I didn‚Äôt have any analytic background at all either. . I started my industrial career at one of analytic consulting companies, Opera Solutions in San Diego, where one of Nicolas‚Äò friends, Jacob, runs the R&amp;D team of the company. Jacob did his Ph.D under the supervision of Prof. Michael Arbib at University of Southern California in Computational Neuroscience as well. During the interview, I was tested to prove my thought process, basic knowledges in statistics and Machine Learning, and programming, which I‚Äôd practiced through out my Ph.D everyday. . So, if he has a good Machine Learning background with programming skills (I‚Äôm sure that he does, based on the fact he‚Äôs your student), he can be competent to pursue his career in Data Science. . Tools in Data Science . Back in the graduate school, I used mostly MATLAB with some SPSS and C. In the Data Science field, Python and R are most popular languages, and SQL is a kind of necessary evil. . R is similar to MATLAB except that it‚Äôs free. It is not a hardcore programming language and doesn‚Äôt take much time to learn. It comes with the latest statistical libraries and provides powerful plotting functions. There are many IDEs, which make easy to use R, but my favorite is R Studio. If you run R on the server with R Studio Server, you can access it from anywhere via your web browser, which is really cool. Although native R plotting functions are excellent by themselves, the ggplot2 library provides more eye-catching visualization. . For Python, Numpy + Scipy packages provides similar vector-matrix computation functionalities as MATLAB. For Machine Learning algorithms, you need Scikit-Learn, and for data handling, Pandas will make your life easy. For debugging and prototyping, iPython Notebook is really handy and useful. . SQL is an old technology but still widely used. Most of data are stored in the data warehouse, which can be accessed only via SQL or SQL equivalents (Oracle, Teradata, Netezza, etc.). Postgres and MySQL are powerful yet free, so it‚Äôs perfect to practice with. . Hints for Kaggle Data Mining Competitions . Fortunately, I had a chance to work with many of top competitors such as the 1st and 2nd place teams at Netflix competitions, and learn how they do at competitions. Here are some tips I found helpful. . Don‚Äôt jump into algorithms too fast. . Spend enough time to understand data. Algorithms are important, but no matter how good algorithm you use, garbage-in only leads to garbage-out. Many classification/regression algorithms assume the Gaussian distributed variables, and fail to make good predictions if you provide non-Gaussian distributed variables. So, standardization, normalization, non-linear transformation, discretization, binning are very important. . Try different algorithms and blend. . There is no universal optimal algorithm. Most of times (if not all), the winning algorithms are ensembles of many individual models with tens of different algorithms. Combining different kinds of models can improve prediction performance a lot. For individual models, I found Random Forest, Gradient Boosting Machine, Factorization Machine, Neural Network, Support Vector Machine, logistic/linear regression, Naive Bayes, and collaborative filtering are mostly useful. Gradient Boosting Machine and Factorization Machine are often the best individual models. . Optimize at last. . Each competition has a different evaluation metric, and optimizing algorithms to do the best for that metric can improve your chance to win. Two most popular metrics are RMSE and AUC (area under the ROC curve). Algorithms optimizing one metric is not the optimal for the other. Many open source algorithm implementations provide only RMSE optimization, so for AUC (or other metric) optimization, you need to implement it by yourself. .",
            "url": "kaggler.com/2013/10/07/ds-career-for-neuroscientist.html",
            "relUrl": "/2013/10/07/ds-career-for-neuroscientist.html",
            "date": " ‚Ä¢ Oct 7, 2013"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "kaggler.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}