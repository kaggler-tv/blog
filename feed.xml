<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="kaggler.com/feed.xml" rel="self" type="application/atom+xml" /><link href="kaggler.com/" rel="alternate" type="text/html" /><updated>2021-05-07T20:24:19-05:00</updated><id>kaggler.com/feed.xml</id><title type="html">Kaggler TV Blog</title><subtitle>Made by Kagglers, for Kagglers. Kaggler TV is a YouTube Channel that shares about machine learning competitions and data science career development.</subtitle><entry><title type="html">TF/Keras BERT Baseline (Training/Inference)</title><link href="kaggler.com/notebook/kaggle/nlp/2021/05/07/tf-keras-bert-baseline-training-inference.html" rel="alternate" type="text/html" title="TF/Keras BERT Baseline (Training/Inference)" /><published>2021-05-07T00:00:00-05:00</published><updated>2021-05-07T00:00:00-05:00</updated><id>kaggler.com/notebook/kaggle/nlp/2021/05/07/tf-keras-bert-baseline-training-inference</id><author><name></name></author><category term="notebook" /><category term="kaggle" /><category term="nlp" /><summary type="html"></summary></entry><entry><title type="html">PyTorch Lightning RoBERTa Baseline (Training/Inference)</title><link href="kaggler.com/notebook/kaggle/nlp/2021/05/07/pytorch-lightning-roberta-baseline.html" rel="alternate" type="text/html" title="PyTorch Lightning RoBERTa Baseline (Training/Inference)" /><published>2021-05-07T00:00:00-05:00</published><updated>2021-05-07T00:00:00-05:00</updated><id>kaggler.com/notebook/kaggle/nlp/2021/05/07/pytorch-lightning-roberta-baseline</id><author><name></name></author><category term="notebook" /><category term="kaggle" /><category term="nlp" /><summary type="html"></summary></entry><entry><title type="html">Kaggler v0.9.4 Release with Stacked DAE</title><link href="kaggler.com/kaggler/dae/2021/05/01/kaggler-new-release.html" rel="alternate" type="text/html" title="Kaggler v0.9.4 Release with Stacked DAE" /><published>2021-05-01T00:00:00-05:00</published><updated>2021-05-01T00:00:00-05:00</updated><id>kaggler.com/kaggler/dae/2021/05/01/kaggler-new-release</id><author><name></name></author><category term="Kaggler" /><category term="DAE" /><summary type="html">Today, Kaggler v0.9.4 is released with additional features for DAE as follows: In addition to the swap noise (swap_prob), the Gaussian noise (noise_std) and zero masking (mask_prob) have been added to DAE to overcome overfitting. Stacked DAE is available through the n_layer input argument (see Figure 3. in Vincent et al. (2010), “Stacked Denoising Autoencoders” for reference).</summary></entry><entry><title type="html">DAE with 2 Lines of Code with Kaggler</title><link href="kaggler.com/notebook/kaggle/2021/04/29/dae-with-2-lines-of-code-with-kaggler.html" rel="alternate" type="text/html" title="DAE with 2 Lines of Code with Kaggler" /><published>2021-04-29T00:00:00-05:00</published><updated>2021-04-29T00:00:00-05:00</updated><id>kaggler.com/notebook/kaggle/2021/04/29/dae-with-2-lines-of-code-with-kaggler</id><author><name></name></author><category term="notebook" /><category term="kaggle" /><summary type="html"></summary></entry><entry><title type="html">Stacking Ensemble</title><link href="kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html" rel="alternate" type="text/html" title="Stacking Ensemble" /><published>2021-04-26T00:00:00-05:00</published><updated>2021-04-26T00:00:00-05:00</updated><id>kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble</id><author><name></name></author><category term="notebook" /><category term="kaggle" /><summary type="html"></summary></entry></feed>